{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1186e0-ccd5-4399-85ea-9363aa0764a1",
   "metadata": {},
   "source": [
    "# Introduction to Big Data Project - Main Python File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d97d2c-6d10-48d4-8b3e-881708699f14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Adespotos\\\\anaconda3\\\\envs\\\\bigdata_env\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that jupyter lab points to your project's environment directory\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be70bf-1b04-43dc-a384-fa54dec40ea1",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5b17b7-1263-432d-97a7-77d354fe98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Numerical computing library\n",
    "import pandas as pd  # Data handling\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import seaborn as sns  # An updated matplotlib (for better visualizations)\n",
    "import pymysql  # MySQL database connector for Python\n",
    "import chardet  # Library for automatic character encoding detection\n",
    "import re  # Regular expressions for string manipulation and pattern matching\n",
    "\n",
    "import custom_functions  # Custom functions for this project (e.g., CSV cleaning, encoding detection)\n",
    "\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6adff-4d96-42e1-8c4e-e649c40c557b",
   "metadata": {},
   "source": [
    "# Read Dataset to a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e5099-9da5-48ab-9e0a-27dbca7dbcf8",
   "metadata": {},
   "source": [
    "The first element of list_of_filepaths corresponds to the first element of the abbreviations list below. These lists make automation easier and help map files to their abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "756ef0e9-dd7e-4e16-8a3e-533ad1582d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of filepaths to our indicators data\n",
    "filepaths_1 = ['UPDATED CSV DATA - Intro to Big Data/Current health expenditure (% of GDP).csv',\n",
    "               'UPDATED CSV DATA - Intro to Big Data/People using at least basic drinking water services, rural (% of rural population).csv',\n",
    "               'UPDATED CSV DATA - Intro to Big Data/People using at least basic drinking water services, urban (% of urban population).csv',\n",
    "               'UPDATED CSV DATA - Intro to Big Data/People using safely managed sanitation services, rural (% of rural population).csv',\n",
    "               'UPDATED CSV DATA - Intro to Big Data/People using safely managed sanitation services, urban (% of urban population).csv',\n",
    "               'UPDATED CSV DATA - Intro to Big Data/Total greenhouse gas emissions including LULUCF (Mt CO2e).csv']\n",
    "\n",
    "filepaths_2 = ['UPDATED CSV DATA - Intro to Big Data/Population, total.csv',\n",
    "               'UPDATED CSV DATA - Intro to Big Data/Renewable energy consumption (% of total final energy consumption).csv']\n",
    "\n",
    "# Custom abbreviation lists for mapping with each dataset\n",
    "abbreviations_1 = ['che', 'wr', 'wu', 'sr', 'su', 'gem']\n",
    "\n",
    "abbreviations_2 = ['pop', 'ren']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d9a10a-ea74-40a6-b5e9-f7d211695f85",
   "metadata": {},
   "source": [
    "A detailed exploration of the CSV files is necessary to ensure they can be read correctly into DataFrames.\n",
    "\n",
    "We selected five health indicators and three environmental indicators. Some of these files are too complex to be read directly into DataFrames. The complexity became apparent when we realized that pandas.read_csv() could not handle the files properly, producing unreliable results.\n",
    "\n",
    "Rather than choosing easier files, we took this as an opportunity to develop a step-by-step solution and learn as much as possible from the process. The Pythonic approach we followed was:\n",
    "\n",
    "1. Ensure the correct file encoding.\n",
    "2. Fetch the CSV file into Pythonâ€™s runtime (without using pandas, since it could not read the file correctly).\n",
    "3. Explore the fetched CSV content to identify the issues.\n",
    "4. Write scripts to address one problem at a time, progressing toward the final solution.\n",
    "\n",
    "The indicators that required this special treatment were **\"Population, total\"** and **\"Renewable energy consumption (% of total final energy consumption)\"**.\n",
    "\n",
    "To handle this process, we created three custom functions for reading and cleaning the CSV files. We also included docstrings in our functions to improve their readability and understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e915cf-952a-4e18-b87e-755c7c1a4ac0",
   "metadata": {},
   "source": [
    "### Step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41afcad-98f7-452d-a0d1-bbc84f862b9d",
   "metadata": {},
   "source": [
    "We use a function to detect the file encoding, ensuring that no encoding issues occur when opening the CSV files in Python, which could otherwise disrupt our approach. We automated the process by iterating simultaneously over the abbreviations and list_of_filepaths lists. This type of iteration ensures that when Python processes element 0 of abbreviations, it also processes element 0 of list_of_filepaths, so each abbreviation corresponds to the correct file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd70997e-ec74-451c-89e1-544862a20d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the files' encodings: \n",
      " {'che': 'UTF-8-SIG', 'wr': 'UTF-8-SIG', 'wu': 'UTF-8-SIG', 'sr': 'UTF-8-SIG', 'su': 'UTF-8-SIG', 'gem': 'UTF-8-SIG'}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store detected encodings for each CSV file\n",
    "encodings_1 = {}\n",
    "\n",
    "# Iterate through abbreviations and filepaths together, detect encoding for each file\n",
    "for abbreviation, filepath in zip(abbreviations_1, filepaths_1):\n",
    "    encodings_1[abbreviation] = custom_functions.detect_encoding(filepath)\n",
    "\n",
    "# Print the results to verify encodings\n",
    "print(\"Here are the files' encodings: \\n\", encodings_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42858863-c5ab-497b-a4f1-3214a528fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the files' encodings: \n",
      " {'pop': 'UTF-8-SIG', 'ren': 'UTF-8-SIG'}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store detected encodings for each CSV file\n",
    "encodings_2 = {}\n",
    "\n",
    "# Iterate through abbreviations and filepaths together, detect encoding for each file\n",
    "for abbreviation, filepath in zip(abbreviations_2, filepaths_2):\n",
    "    encodings_2[abbreviation] = custom_functions.detect_encoding(filepath)\n",
    "\n",
    "# Print the results to verify encodings\n",
    "print(\"Here are the files' encodings: \\n\", encodings_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a1f98-0434-48c3-8b7c-09f1eaeb0292",
   "metadata": {},
   "source": [
    "At this point, all encodings are safely stored as values in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb99f19f-9cd0-485d-9306-0dbe091f7b75",
   "metadata": {},
   "source": [
    "### Step 2 & 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2204ab-eebd-4557-a313-197e83b6c804",
   "metadata": {},
   "source": [
    "Standard Python can be used to load parts of a CSV file into memory. We can automate this process in the same way as before and display a line from each CSV file directly within a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35485635-b998-4b91-903b-759bed311bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "che:\n",
      "'Country Name;Country Code;Indicator Name;Indicator Code;1960;1961;1962;1963;1964;1965;1966;1967;1968;1969;1970;1971;1972;1973;1974;1975;1976;1977;1978;1979;1980;1981;1982;1983;1984;1985;1986;1987;1988;1989;1990;1991;1992;1993;1994;1995;1996;1997;1998;1999;2000;2001;2002;2003;2004;2005;2006;2007;2008;2009;2010;2011;2012;2013;2014;2015;2016;2017;2018;2019;2020;2021;2022;2023;2024\\n'\n",
      "wr:\n",
      "'Country Name;Country Code;Indicator Name;Indicator Code;1960;1961;1962;1963;1964;1965;1966;1967;1968;1969;1970;1971;1972;1973;1974;1975;1976;1977;1978;1979;1980;1981;1982;1983;1984;1985;1986;1987;1988;1989;1990;1991;1992;1993;1994;1995;1996;1997;1998;1999;2000;2001;2002;2003;2004;2005;2006;2007;2008;2009;2010;2011;2012;2013;2014;2015;2016;2017;2018;2019;2020;2021;2022;2023;2024\\n'\n",
      "wu:\n",
      "'Country Name;Country Code;Indicator Name;Indicator Code;1960;1961;1962;1963;1964;1965;1966;1967;1968;1969;1970;1971;1972;1973;1974;1975;1976;1977;1978;1979;1980;1981;1982;1983;1984;1985;1986;1987;1988;1989;1990;1991;1992;1993;1994;1995;1996;1997;1998;1999;2000;2001;2002;2003;2004;2005;2006;2007;2008;2009;2010;2011;2012;2013;2014;2015;2016;2017;2018;2019;2020;2021;2022;2023;2024\\n'\n",
      "sr:\n",
      "'Country Name;Country Code;Indicator Name;Indicator Code;1960;1961;1962;1963;1964;1965;1966;1967;1968;1969;1970;1971;1972;1973;1974;1975;1976;1977;1978;1979;1980;1981;1982;1983;1984;1985;1986;1987;1988;1989;1990;1991;1992;1993;1994;1995;1996;1997;1998;1999;2000;2001;2002;2003;2004;2005;2006;2007;2008;2009;2010;2011;2012;2013;2014;2015;2016;2017;2018;2019;2020;2021;2022;2023;2024\\n'\n",
      "su:\n",
      "'Country Name;Country Code;Indicator Name;Indicator Code;1960;1961;1962;1963;1964;1965;1966;1967;1968;1969;1970;1971;1972;1973;1974;1975;1976;1977;1978;1979;1980;1981;1982;1983;1984;1985;1986;1987;1988;1989;1990;1991;1992;1993;1994;1995;1996;1997;1998;1999;2000;2001;2002;2003;2004;2005;2006;2007;2008;2009;2010;2011;2012;2013;2014;2015;2016;2017;2018;2019;2020;2021;2022;2023;2024\\n'\n",
      "gem:\n",
      "'Country Name;Country Code;Indicator Name;Indicator Code;1960;1961;1962;1963;1964;1965;1966;1967;1968;1969;1970;1971;1972;1973;1974;1975;1976;1977;1978;1979;1980;1981;1982;1983;1984;1985;1986;1987;1988;1989;1990;1991;1992;1993;1994;1995;1996;1997;1998;1999;2000;2001;2002;2003;2004;2005;2006;2007;2008;2009;2010;2011;2012;2013;2014;2015;2016;2017;2018;2019;2020;2021;2022;2023;2024\\n'\n"
     ]
    }
   ],
   "source": [
    "# Iterate through abbreviations and filepaths together, print a CSV line for each file\n",
    "for abbreviation, filepath in zip(abbreviations_1, filepaths_1):\n",
    "    csv_part = custom_functions.explore_csv(filepath=filepath, encoding=encodings_1[abbreviation], line_number=3)\n",
    "    print(abbreviation + \":\")\n",
    "    print(repr(csv_part))  # repr() displays the string exactly as it is, unlike print() which ruins special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e851655b-a918-41c8-8b05-7aee8799b54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop:\n",
      "'\"Country Name,\"\"Country Code\"\",\"\"Indicator Name\"\",\"\"Indicator Code\"\",\"\"1960\"\",\"\"1961\"\",\"\"1962\"\",\"\"1963\"\",\"\"1964\"\",\"\"1965\"\",\"\"1966\"\",\"\"1967\"\",\"\"1968\"\",\"\"1969\"\",\"\"1970\"\",\"\"1971\"\",\"\"1972\"\",\"\"1973\"\",\"\"1974\"\",\"\"1975\"\",\"\"1976\"\",\"\"1977\"\",\"\"1978\"\",\"\"1979\"\",\"\"1980\"\",\"\"1981\"\",\"\"1982\"\",\"\"1983\"\",\"\"1984\"\",\"\"1985\"\",\"\"1986\"\",\"\"1987\"\",\"\"1988\"\",\"\"1989\"\",\"\"1990\"\",\"\"1991\"\",\"\"1992\"\",\"\"1993\"\",\"\"1994\"\",\"\"1995\"\",\"\"1996\"\",\"\"1997\"\",\"\"1998\"\",\"\"1999\"\",\"\"2000\"\",\"\"2001\"\",\"\"2002\"\",\"\"2003\"\",\"\"2004\"\",\"\"2005\"\",\"\"2006\"\",\"\"2007\"\",\"\"2008\"\",\"\"2009\"\",\"\"2010\"\",\"\"2011\"\",\"\"2012\"\",\"\"2013\"\",\"\"2014\"\",\"\"2015\"\",\"\"2016\"\",\"\"2017\"\",\"\"2018\"\",\"\"2019\"\",\"\"2020\"\",\"\"2021\"\",\"\"2022\"\",\"\"2023\"\",\"\"2024\"\",\"\\n'\n",
      "ren:\n",
      "'\"Country Name,\"\"Country Code\"\",\"\"Indicator Name\"\",\"\"Indicator Code\"\",\"\"1960\"\",\"\"1961\"\",\"\"1962\"\",\"\"1963\"\",\"\"1964\"\",\"\"1965\"\",\"\"1966\"\",\"\"1967\"\",\"\"1968\"\",\"\"1969\"\",\"\"1970\"\",\"\"1971\"\",\"\"1972\"\",\"\"1973\"\",\"\"1974\"\",\"\"1975\"\",\"\"1976\"\",\"\"1977\"\",\"\"1978\"\",\"\"1979\"\",\"\"1980\"\",\"\"1981\"\",\"\"1982\"\",\"\"1983\"\",\"\"1984\"\",\"\"1985\"\",\"\"1986\"\",\"\"1987\"\",\"\"1988\"\",\"\"1989\"\",\"\"1990\"\",\"\"1991\"\",\"\"1992\"\",\"\"1993\"\",\"\"1994\"\",\"\"1995\"\",\"\"1996\"\",\"\"1997\"\",\"\"1998\"\",\"\"1999\"\",\"\"2000\"\",\"\"2001\"\",\"\"2002\"\",\"\"2003\"\",\"\"2004\"\",\"\"2005\"\",\"\"2006\"\",\"\"2007\"\",\"\"2008\"\",\"\"2009\"\",\"\"2010\"\",\"\"2011\"\",\"\"2012\"\",\"\"2013\"\",\"\"2014\"\",\"\"2015\"\",\"\"2016\"\",\"\"2017\"\",\"\"2018\"\",\"\"2019\"\",\"\"2020\"\",\"\"2021\"\",\"\"2022\"\",\"\"2023\"\",\"\"2024\"\",\"\\n'\n"
     ]
    }
   ],
   "source": [
    "# Iterate through abbreviations and filepaths together, print a CSV line for each file\n",
    "for abbreviation, filepath in zip(abbreviations_2, filepaths_2):\n",
    "    csv_part = custom_functions.explore_csv(filepath=filepath, encoding=encodings_2[abbreviation], line_number=4)\n",
    "    print(abbreviation + \":\")\n",
    "    print(repr(csv_part))  # repr() displays the string exactly as it is, unlike print() which ruins special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b9fb0-7fee-447f-8da8-8f59176e2b40",
   "metadata": {},
   "source": [
    "The above scripts, along with the custom function explore_csv, helped us identify the following:\n",
    "\n",
    "1. Files 'che', 'wr', 'wu', 'sr', 'su', and 'gem' start at line number 3, whereas 'pop' and 'ren' start at line number 4. The preceding lines contain metadata.\n",
    "2. Files 'che', 'wr', 'wu', 'sr', 'su', and 'gem' use ';' as a separator, whereas 'pop' and 'ren' use ','.\n",
    "3. Rows in every file end with a newline character '\\n'.\n",
    "4. Apart from the delimiter and the newline character at the end, the files 'che', 'wr', 'wu', 'sr', 'su', and 'gem' are clean, while 'pop' and 'ren' include the following extra issues:  \n",
    "    a. Each row ends with a comma followed by a quote, before the newline character: ',\"\\n'.  \n",
    "    b. Quotes included in the data.  \n",
    "    c. In 'pop', there is an extra comma in the column name 'Population, total', which causes pandas to treat 'Population' and 'total' as separate columns, even though they belong to the same column.  \n",
    "    d. Each list element represents a row of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb55a5fa-bdc0-40d8-9cab-ea08dc835436",
   "metadata": {},
   "source": [
    "**NOTE: If we remove quotes before applying rstrip to the newline character, the file 'ren' may not be read correctly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0114c1e2-3047-4251-80cd-dba76ed849f1",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e151ce7-7ac0-4c6f-bb0d-b06cb357bb34",
   "metadata": {},
   "source": [
    "We created a custom function to clean all files. The function is dynamic, meaning it can clean the datasets 'che', 'wr', 'wu', 'sr', 'su', 'gem', as well as 'pop' and 'ren'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba3b22-6fbe-4608-9f5e-ac963f5af21b",
   "metadata": {},
   "source": [
    "### Reading 'che', 'wr', 'wu', 'sr', 'su', 'gem' to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cc2b8bb-45f9-4a32-9176-21b05e934b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnames_1 = ['df_che', 'df_wr', 'df_wu', 'df_sr', 'df_su', 'df_gem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bd9df30-0eb0-4054-bc0e-ec1534904844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_1 = {}\n",
    "\n",
    "for df_name, abbreviation, filepath in zip(dfnames_1, abbreviations_1, filepaths_1):\n",
    "    df_dict_1[df_name] = custom_functions.clean_csv(\n",
    "        filepath=filepath,\n",
    "        encoding=encodings_1[abbreviation],\n",
    "        separator=';',\n",
    "        trail1='\\n',\n",
    "        trail2=None,\n",
    "        trail3=None,\n",
    "        to_be_replaced='\"',\n",
    "        start_row=3\n",
    "    )\n",
    "    \n",
    "df_che = df_dict_1['df_che']\n",
    "df_wr = df_dict_1['df_wr']\n",
    "df_wu = df_dict_1['df_wu']\n",
    "df_sr = df_dict_1['df_sr']\n",
    "df_su = df_dict_1['df_su']\n",
    "df_gem = df_dict_1['df_gem']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d1cdfc-af08-4a00-9f51-65f87c09794a",
   "metadata": {},
   "source": [
    "### Reading 'pop' and 'ren' to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a382c21c-20f2-464c-b7bf-72e71b0f1db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnames_2 = ['df_pop', 'df_ren']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adc6b8bf-bcf6-47db-83f8-120b75fa96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_2 = {}\n",
    "\n",
    "for df_name, abbreviation, filepath in zip(dfnames_2, abbreviations_2, filepaths_2):\n",
    "    df_dict_2[df_name] = custom_functions.clean_csv(\n",
    "        filepath=filepath,\n",
    "        encoding=encodings_2[abbreviation],\n",
    "        separator=',',\n",
    "        trail1='\\n',\n",
    "        trail2='\"',\n",
    "        trail3=',',\n",
    "        to_be_replaced='\"',\n",
    "        start_row=4\n",
    "    )\n",
    "    \n",
    "df_pop = df_dict_2['df_pop']\n",
    "df_ren = df_dict_2['df_ren']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232370bf-ce5d-49a8-8202-99be6ae051a3",
   "metadata": {},
   "source": [
    "# Quick Exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "804fd065-d860-4c3b-b19a-1700cf979906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "che: (266, 69)\n",
      "wr: (266, 69)\n",
      "wu: (266, 69)\n",
      "sr: (266, 69)\n",
      "su: (266, 69)\n",
      "pop: (266, 69)\n",
      "ren: (266, 69)\n",
      "gem: (266, 69)\n"
     ]
    }
   ],
   "source": [
    "print('che:', df_che.shape)\n",
    "print('wr:', df_wr.shape)\n",
    "print('wu:', df_wu.shape)\n",
    "print('sr:', df_sr.shape)\n",
    "print('su:', df_su.shape)\n",
    "print('pop:', df_pop.shape)\n",
    "print('ren:', df_ren.shape)\n",
    "print('gem:', df_gem.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
