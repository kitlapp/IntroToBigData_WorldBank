{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1186e0-ccd5-4399-85ea-9363aa0764a1",
   "metadata": {},
   "source": [
    "# Introduction to Big Data Project - Main Python File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d97d2c-6d10-48d4-8b3e-881708699f14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Adespotos\\\\anaconda3\\\\envs\\\\bigdata_env\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that jupyter lab points to your project's environment directory\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be70bf-1b04-43dc-a384-fa54dec40ea1",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5b17b7-1263-432d-97a7-77d354fe98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Numerical computing library\n",
    "import pandas as pd  # Data handling\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import seaborn as sns  # An updated matplotlib (for better visualizations)\n",
    "import pymysql  # MySQL database connector for Python\n",
    "import chardet  # Library for automatic character encoding detection\n",
    "import re  # Regular expressions for string manipulation and pattern matching\n",
    "\n",
    "import custom_functions  # Custom functions for this project (e.g., CSV cleaning, encoding detection)\n",
    "\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6ff14-85b8-467d-b7fe-40fbfb144309",
   "metadata": {},
   "source": [
    "# DATA HARVESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6adff-4d96-42e1-8c4e-e649c40c557b",
   "metadata": {},
   "source": [
    "# Read Datasets to DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d9a10a-ea74-40a6-b5e9-f7d211695f85",
   "metadata": {},
   "source": [
    "A detailed exploration of the CSV files is necessary to ensure they can be read correctly into DataFrames.\n",
    "\n",
    "We selected five health indicators and three environmental indicators. Some of these files are too complex to be read directly into DataFrames. The complexity became apparent when we realized that pandas.read_csv() could not handle the files properly, producing unreliable results.\n",
    "\n",
    "Rather than choosing easier files, we took this as an opportunity to develop a step-by-step solution and learn as much as possible from the process. The Pythonic approach we followed includes the steps below:\n",
    "\n",
    "1. Ensure the correct file encoding.\n",
    "2. Fetch the CSV files into Python’s runtime (without using pandas, since it could not read the files correctly).\n",
    "3. Explore the fetched CSV content to identify the issues.\n",
    "4. Write scripts to address one problem at a time, progressing toward the final solution.\n",
    "\n",
    "The indicators that required this special treatment were **\"Population, total\"** and **\"Renewable energy consumption (% of total final energy consumption)\"**. For this reason, we separated these two files from the rest, assigning them the notation 2, while the remaining files use the notation 1.\n",
    "\n",
    "To handle the process described above, we also created three custom functions for reading and cleaning the CSV files. We also included docstrings in our functions to improve their readability and understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e5099-9da5-48ab-9e0a-27dbca7dbcf8",
   "metadata": {},
   "source": [
    "Therefore, as the first preprocessing step, we create the lists with file paths and data abbreviations. The mechanics are simple: the first element of filepaths_1 corresponds to the first element of abbreviations_1. The same applies to filepaths_2 and abbreviations_2. So, the custom creation order of the list elements matters. These lists make automation easier and help map each file to its abbreviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "756ef0e9-dd7e-4e16-8a3e-533ad1582d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for notation 1 ('che', 'wr', 'wu', 'sr', 'su', 'gem')\n",
    "filepaths_1 = ['UPDATED CSV DATA - Intro to Big Data/Current health expenditure (% of GDP).csv',\n",
    "               'UPDATED CSV DATA - Intro to Big Data/People using at least basic drinking water services, rural (% of rural population).csv',\n",
    "               'UPDATED CSV DATA - Intro to Big Data/People using at least basic drinking water services, urban (% of urban population).csv',\n",
    "               'UPDATED CSV DATA - Intro to Big Data/People using safely managed sanitation services, rural (% of rural population).csv',\n",
    "               'UPDATED CSV DATA - Intro to Big Data/People using safely managed sanitation services, urban (% of urban population).csv',\n",
    "               'UPDATED CSV DATA - Intro to Big Data/Total greenhouse gas emissions including LULUCF (Mt CO2e).csv']\n",
    "\n",
    "# File paths for notation 2 ('pop', 'ren')\n",
    "filepaths_2 = ['UPDATED CSV DATA - Intro to Big Data/Population, total.csv',\n",
    "               'UPDATED CSV DATA - Intro to Big Data/Renewable energy consumption (% of total final energy consumption).csv']\n",
    "\n",
    "# Abbreviations for easy mapping: notation 1\n",
    "abbreviations_1 = ['che', 'wr', 'wu', 'sr', 'su', 'gem']\n",
    "\n",
    "# Abbreviations for easy mapping: notation 2\n",
    "abbreviations_2 = ['pop', 'ren']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e915cf-952a-4e18-b87e-755c7c1a4ac0",
   "metadata": {},
   "source": [
    "### Step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41afcad-98f7-452d-a0d1-bbc84f862b9d",
   "metadata": {},
   "source": [
    "We use a function, leveraging chardet library's capabilities, to detect the file encoding, ensuring that no encoding issues occur when opening the CSV files in Python, which could otherwise disrupt our approach. We automated the process by iterating simultaneously over the abbreviations and filepaths lists. This type of iteration ensures that when Python processes element 0 of abbreviations, it also processes element 0 of filepaths, so each abbreviation corresponds to the correct file path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0266c6d-5561-4634-b19b-52b4d2d245ef",
   "metadata": {},
   "source": [
    "**Notation 1 Files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd70997e-ec74-451c-89e1-544862a20d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the files' encodings: \n",
      " {'che': 'UTF-8-SIG', 'wr': 'UTF-8-SIG', 'wu': 'UTF-8-SIG', 'sr': 'UTF-8-SIG', 'su': 'UTF-8-SIG', 'gem': 'UTF-8-SIG'}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store detected encodings for each CSV file\n",
    "encodings_1 = {}\n",
    "\n",
    "# Iterate through abbreviations and filepaths together, detect encoding for each file\n",
    "for abbreviation, filepath in zip(abbreviations_1, filepaths_1):\n",
    "    encodings_1[abbreviation] = custom_functions.detect_encoding(filepath)\n",
    "\n",
    "# Print the results to verify encodings\n",
    "print(\"Here are the files' encodings: \\n\", encodings_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4210b0-b85b-4050-9f49-378331d30bb7",
   "metadata": {},
   "source": [
    "**Notation 2 Files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42858863-c5ab-497b-a4f1-3214a528fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the files' encodings: \n",
      " {'pop': 'UTF-8-SIG', 'ren': 'UTF-8-SIG'}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store detected encodings for each CSV file\n",
    "encodings_2 = {}\n",
    "\n",
    "# Iterate through abbreviations and filepaths together, detect encoding for each file\n",
    "for abbreviation, filepath in zip(abbreviations_2, filepaths_2):\n",
    "    encodings_2[abbreviation] = custom_functions.detect_encoding(filepath)\n",
    "\n",
    "# Print the results to verify encodings\n",
    "print(\"Here are the files' encodings: \\n\", encodings_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a1f98-0434-48c3-8b7c-09f1eaeb0292",
   "metadata": {},
   "source": [
    "At this point, all encodings are safely stored as values in a dictionary. Things seem simple because the encodings are currently the same. However, if they ever differ, each encoding can be accessed using the corresponding dictionary key. Even though the situation looks straightforward now, we follow a dynamic approach and retrieve the encodings through the dictionary keys instead of writing the encoding manually for each file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb99f19f-9cd0-485d-9306-0dbe091f7b75",
   "metadata": {},
   "source": [
    "### Step 2 & 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2204ab-eebd-4557-a313-197e83b6c804",
   "metadata": {},
   "source": [
    "Since Pandas cannot handle reading these CSV files directly, we can use standard Python to load portions of a CSV file into memory. We can automate this process as before by iterating through both the filepaths and abbreviations lists and using a custom function that returns a selected line from each CSV file. We also use the repr() function, which is very helpful in situations like this, since the print() function often alters the displayed content. It is important to load the original CSV lines into memory without any changes to correctly identify the necessary actions to clean the files so that Pandas can read them successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ad6b9-70e2-4d05-a789-2de7a7e2996b",
   "metadata": {},
   "source": [
    "**Notation 1 Files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35485635-b998-4b91-903b-759bed311bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "che:\n",
      "'Country Name;Country Code;Indicator Name;Indicator Code;1960;1961;1962;1963;1964;1965;1966;1967;1968;1969;1970;1971;1972;1973;1974;1975;1976;1977;1978;1979;1980;1981;1982;1983;1984;1985;1986;1987;1988;1989;1990;1991;1992;1993;1994;1995;1996;1997;1998;1999;2000;2001;2002;2003;2004;2005;2006;2007;2008;2009;2010;2011;2012;2013;2014;2015;2016;2017;2018;2019;2020;2021;2022;2023;2024\\n'\n",
      "wr:\n",
      "'Country Name;Country Code;Indicator Name;Indicator Code;1960;1961;1962;1963;1964;1965;1966;1967;1968;1969;1970;1971;1972;1973;1974;1975;1976;1977;1978;1979;1980;1981;1982;1983;1984;1985;1986;1987;1988;1989;1990;1991;1992;1993;1994;1995;1996;1997;1998;1999;2000;2001;2002;2003;2004;2005;2006;2007;2008;2009;2010;2011;2012;2013;2014;2015;2016;2017;2018;2019;2020;2021;2022;2023;2024\\n'\n",
      "wu:\n",
      "'Country Name;Country Code;Indicator Name;Indicator Code;1960;1961;1962;1963;1964;1965;1966;1967;1968;1969;1970;1971;1972;1973;1974;1975;1976;1977;1978;1979;1980;1981;1982;1983;1984;1985;1986;1987;1988;1989;1990;1991;1992;1993;1994;1995;1996;1997;1998;1999;2000;2001;2002;2003;2004;2005;2006;2007;2008;2009;2010;2011;2012;2013;2014;2015;2016;2017;2018;2019;2020;2021;2022;2023;2024\\n'\n",
      "sr:\n",
      "'Country Name;Country Code;Indicator Name;Indicator Code;1960;1961;1962;1963;1964;1965;1966;1967;1968;1969;1970;1971;1972;1973;1974;1975;1976;1977;1978;1979;1980;1981;1982;1983;1984;1985;1986;1987;1988;1989;1990;1991;1992;1993;1994;1995;1996;1997;1998;1999;2000;2001;2002;2003;2004;2005;2006;2007;2008;2009;2010;2011;2012;2013;2014;2015;2016;2017;2018;2019;2020;2021;2022;2023;2024\\n'\n",
      "su:\n",
      "'Country Name;Country Code;Indicator Name;Indicator Code;1960;1961;1962;1963;1964;1965;1966;1967;1968;1969;1970;1971;1972;1973;1974;1975;1976;1977;1978;1979;1980;1981;1982;1983;1984;1985;1986;1987;1988;1989;1990;1991;1992;1993;1994;1995;1996;1997;1998;1999;2000;2001;2002;2003;2004;2005;2006;2007;2008;2009;2010;2011;2012;2013;2014;2015;2016;2017;2018;2019;2020;2021;2022;2023;2024\\n'\n",
      "gem:\n",
      "'Country Name;Country Code;Indicator Name;Indicator Code;1960;1961;1962;1963;1964;1965;1966;1967;1968;1969;1970;1971;1972;1973;1974;1975;1976;1977;1978;1979;1980;1981;1982;1983;1984;1985;1986;1987;1988;1989;1990;1991;1992;1993;1994;1995;1996;1997;1998;1999;2000;2001;2002;2003;2004;2005;2006;2007;2008;2009;2010;2011;2012;2013;2014;2015;2016;2017;2018;2019;2020;2021;2022;2023;2024\\n'\n"
     ]
    }
   ],
   "source": [
    "# Iterate through abbreviations and filepaths together, print a CSV line for each file\n",
    "for abbreviation, filepath in zip(abbreviations_1, filepaths_1):\n",
    "    csv_part = custom_functions.explore_csv(filepath=filepath, encoding=encodings_1[abbreviation], line_number=3)\n",
    "    print(abbreviation + \":\")\n",
    "    print(repr(csv_part))  # repr() displays the string exactly as it is, unlike print() which might ruin special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffcc3f-b9d5-4b7c-9338-1e70f8977399",
   "metadata": {},
   "source": [
    "**Notation 2 Files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e851655b-a918-41c8-8b05-7aee8799b54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop:\n",
      "'\"Country Name,\"\"Country Code\"\",\"\"Indicator Name\"\",\"\"Indicator Code\"\",\"\"1960\"\",\"\"1961\"\",\"\"1962\"\",\"\"1963\"\",\"\"1964\"\",\"\"1965\"\",\"\"1966\"\",\"\"1967\"\",\"\"1968\"\",\"\"1969\"\",\"\"1970\"\",\"\"1971\"\",\"\"1972\"\",\"\"1973\"\",\"\"1974\"\",\"\"1975\"\",\"\"1976\"\",\"\"1977\"\",\"\"1978\"\",\"\"1979\"\",\"\"1980\"\",\"\"1981\"\",\"\"1982\"\",\"\"1983\"\",\"\"1984\"\",\"\"1985\"\",\"\"1986\"\",\"\"1987\"\",\"\"1988\"\",\"\"1989\"\",\"\"1990\"\",\"\"1991\"\",\"\"1992\"\",\"\"1993\"\",\"\"1994\"\",\"\"1995\"\",\"\"1996\"\",\"\"1997\"\",\"\"1998\"\",\"\"1999\"\",\"\"2000\"\",\"\"2001\"\",\"\"2002\"\",\"\"2003\"\",\"\"2004\"\",\"\"2005\"\",\"\"2006\"\",\"\"2007\"\",\"\"2008\"\",\"\"2009\"\",\"\"2010\"\",\"\"2011\"\",\"\"2012\"\",\"\"2013\"\",\"\"2014\"\",\"\"2015\"\",\"\"2016\"\",\"\"2017\"\",\"\"2018\"\",\"\"2019\"\",\"\"2020\"\",\"\"2021\"\",\"\"2022\"\",\"\"2023\"\",\"\"2024\"\",\"\\n'\n",
      "ren:\n",
      "'\"Country Name,\"\"Country Code\"\",\"\"Indicator Name\"\",\"\"Indicator Code\"\",\"\"1960\"\",\"\"1961\"\",\"\"1962\"\",\"\"1963\"\",\"\"1964\"\",\"\"1965\"\",\"\"1966\"\",\"\"1967\"\",\"\"1968\"\",\"\"1969\"\",\"\"1970\"\",\"\"1971\"\",\"\"1972\"\",\"\"1973\"\",\"\"1974\"\",\"\"1975\"\",\"\"1976\"\",\"\"1977\"\",\"\"1978\"\",\"\"1979\"\",\"\"1980\"\",\"\"1981\"\",\"\"1982\"\",\"\"1983\"\",\"\"1984\"\",\"\"1985\"\",\"\"1986\"\",\"\"1987\"\",\"\"1988\"\",\"\"1989\"\",\"\"1990\"\",\"\"1991\"\",\"\"1992\"\",\"\"1993\"\",\"\"1994\"\",\"\"1995\"\",\"\"1996\"\",\"\"1997\"\",\"\"1998\"\",\"\"1999\"\",\"\"2000\"\",\"\"2001\"\",\"\"2002\"\",\"\"2003\"\",\"\"2004\"\",\"\"2005\"\",\"\"2006\"\",\"\"2007\"\",\"\"2008\"\",\"\"2009\"\",\"\"2010\"\",\"\"2011\"\",\"\"2012\"\",\"\"2013\"\",\"\"2014\"\",\"\"2015\"\",\"\"2016\"\",\"\"2017\"\",\"\"2018\"\",\"\"2019\"\",\"\"2020\"\",\"\"2021\"\",\"\"2022\"\",\"\"2023\"\",\"\"2024\"\",\"\\n'\n"
     ]
    }
   ],
   "source": [
    "# Iterate through abbreviations and filepaths together, print a CSV line for each file\n",
    "for abbreviation, filepath in zip(abbreviations_2, filepaths_2):\n",
    "    csv_part = custom_functions.explore_csv(filepath=filepath, encoding=encodings_2[abbreviation], line_number=4)\n",
    "    print(abbreviation + \":\")\n",
    "    print(repr(csv_part))  # repr() displays the string exactly as it is, unlike print() which ruins special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b9fb0-7fee-447f-8da8-8f59176e2b40",
   "metadata": {},
   "source": [
    "The above scripts, along with the custom function explore_csv, helped us identify the following:\n",
    "\n",
    "1. All notation 1 files start at line number 3, whereas notation 2 files start at line number 4. The preceding lines contain metadata at both cases.\n",
    "2. All notation 1 files use ';' as a separator, whereas notation 2 files use ','.\n",
    "3. Rows in every file end with a newline character '\\n'.\n",
    "4. Apart from the delimiter and the newline character at the end, the notation 1 files are clean, while notation 2 files include the following extra issues:  \n",
    "    a. Each row ends with a comma followed by a quote, before the newline character: ',\"\\n'.  \n",
    "    b. Quotes included in the data.  \n",
    "    c. In 'pop', there is an extra comma in the column name 'Population, total', which causes pandas to treat 'Population' and 'total' as separate columns, even though they belong to the same column.  \n",
    "    d. Each list element represents a row of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722778e4-8c07-4fcb-ad76-08006ee5f18e",
   "metadata": {},
   "source": [
    "Although the identification appears to be correct and effective, the order of execution of the above instructions is important. For instance, removing all quotes before applying rstrip() can lead to unreliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0114c1e2-3047-4251-80cd-dba76ed849f1",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e151ce7-7ac0-4c6f-bb0d-b06cb357bb34",
   "metadata": {},
   "source": [
    "We created a custom function to clean all files. The function is dynamic, meaning it can handle both notation 1 and notation 2 datasets. The only point that requires attention is that the function must accept different arguments depending on the file notation. As mentioned before, notation 2 files require special treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8544644-922a-4006-b5a9-4e13d1a5bd6c",
   "metadata": {},
   "source": [
    "We adopted a new approach by creating a third list that maps to the other two lists introduced earlier. This third list contains the names of the DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba3b22-6fbe-4608-9f5e-ac963f5af21b",
   "metadata": {},
   "source": [
    "**Reading Notation 1 Files to DataFrames:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cc2b8bb-45f9-4a32-9176-21b05e934b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the DataFrames for notation 1\n",
    "dfnames_1 = ['df_che', 'df_wr', 'df_wu', 'df_sr', 'df_su', 'df_gem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bd9df30-0eb0-4054-bc0e-ec1534904844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_1 = {}  # Initialize an empty dictionary to store the DataFrames\n",
    "\n",
    "# Iterate through the three lists simultaneously.\n",
    "# This works because the lists were created with the correct mapping order.\n",
    "for df_name, abbreviation, filepath in zip(dfnames_1, abbreviations_1, filepaths_1):\n",
    "    # Use the DataFrame name string as the key in the dictionary\n",
    "    df_dict_1[df_name] = custom_functions.clean_csv(\n",
    "        filepath=filepath,  # Path to the CSV file\n",
    "        encoding=encodings_1[abbreviation],  # Retrieve encoding based on abbreviation\n",
    "        separator=';',   # Set the column delimiter\n",
    "        trail1='\\n',  # First trailing character to remove\n",
    "        trail2=None,  # Second trailing character to remove (optional)\n",
    "        trail3=None,  # Third trailing character to remove (optional)\n",
    "        to_be_replaced='\"',  # Characters to replace (quotes in this case)\n",
    "        start_row=3  # Row index corresponding to column headers\n",
    "    )\n",
    "\n",
    "# Extract the DataFrames from the dictionary and assign to variables\n",
    "df_che = df_dict_1['df_che']\n",
    "df_wr  = df_dict_1['df_wr']\n",
    "df_wu  = df_dict_1['df_wu']\n",
    "df_sr  = df_dict_1['df_sr']\n",
    "df_su  = df_dict_1['df_su']\n",
    "df_gem = df_dict_1['df_gem']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d1cdfc-af08-4a00-9f51-65f87c09794a",
   "metadata": {},
   "source": [
    "**Reading Notation 2 Files to DataFrames:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a382c21c-20f2-464c-b7bf-72e71b0f1db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the DataFrames for notation 2\n",
    "dfnames_2 = ['df_pop', 'df_ren']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adc6b8bf-bcf6-47db-83f8-120b75fa96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_2 = {}\n",
    "\n",
    "for df_name, abbreviation, filepath in zip(dfnames_2, abbreviations_2, filepaths_2):\n",
    "    df_dict_2[df_name] = custom_functions.clean_csv(\n",
    "        filepath=filepath,\n",
    "        encoding=encodings_2[abbreviation],\n",
    "        separator=',',\n",
    "        trail1='\\n',\n",
    "        trail2='\"',\n",
    "        trail3=',',\n",
    "        to_be_replaced='\"',\n",
    "        start_row=4\n",
    "    )\n",
    "    \n",
    "df_pop = df_dict_2['df_pop']\n",
    "df_ren = df_dict_2['df_ren']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232370bf-ce5d-49a8-8202-99be6ae051a3",
   "metadata": {},
   "source": [
    "# Quick Exploration on Data Integrity & Observations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6de22d-8ad0-4b6c-aee7-5f6dabbae451",
   "metadata": {},
   "source": [
    "The results appear reliable, as all DataFrames have the same shape and identical column names. A quick inspection suggests that our cleaning process has been effective. All indicators are related to countries, so the 266 rows represent countries and territories worldwide. The 193 sovereign countries are included, but the additional rows correspond to non-independent territories, overseas dependencies and entire regions, such as Puerto Rico, Hong Kong, Bermuda, Africa Eastern and Southern. Arab World etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f34bd-0781-4dd8-954f-7c9d6348f2aa",
   "metadata": {},
   "source": [
    "The indicators’ data have been collected from 1960 to the present (2024). There are several reasons why 1960 is used as the starting year. By this time, most countries had rebuilt or established functioning statistical offices after World War II, enabling systematic and comparable data collection. Additionally, decolonization and the formation of new countries occurred primarily in the 1950s and 1960s. Many nations became independent around this period, so data prior to 1960 would often be incomplete, inconsistent, or recorded under colonial administrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804fd065-d860-4c3b-b19a-1700cf979906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('che:', df_che.shape)\n",
    "print('wr:', df_wr.shape)\n",
    "print('wu:', df_wu.shape)\n",
    "print('sr:', df_sr.shape)\n",
    "print('su:', df_su.shape)\n",
    "print('pop:', df_pop.shape)\n",
    "print('ren:', df_ren.shape)\n",
    "print('gem:', df_gem.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f361ce-26e1-4a0f-a214-6b65554dac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"che Columns:\\n\", df_che.columns)\n",
    "print(\"wr Columns:\\n\", df_wr.columns)\n",
    "print(\"wu Columns:\\n\", df_wu.columns)\n",
    "print(\"sr Columns:\\n\", df_sr.columns)\n",
    "print(\"su Columns:\\n\", df_su.columns)\n",
    "print(\"pop Columns:\\n\", df_pop.columns)\n",
    "print(\"ren Columns:\\n\", df_ren.columns)\n",
    "print(\"gem Columns:\\n\", df_gem.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "770332d5-c32f-4341-a233-a9e469f8dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all unique countries of the datasets\n",
    "df_che['Country Name'].unique();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85dede9-d7ce-418a-befc-eb1f12f0d42a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
