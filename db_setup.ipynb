{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b21173c9",
   "metadata": {},
   "source": [
    "# Database Setup Notebook\n",
    "\n",
    "This notebook builds the entire MySQL database for the Big Data project.\n",
    "\n",
    "The steps performed:\n",
    "\n",
    "- [Step 0 ‚Äî Import Libraries](#step20)\n",
    "- [Step 1 ‚Äî Define File Paths & Abbreviations](#step21)\n",
    "- [Step 2 ‚Äî Detect Encodings](#step22)\n",
    "- [Step 3 ‚Äî Clean CSV Files](#step23)\n",
    "- [Step 4 ‚Äî Convert to Long Format (`all_long`)](#step24)\n",
    "- [Step 5 ‚Äî Build `countries_df`](#step25)\n",
    "- [Step 6 ‚Äî Build `indicators_df`](#step26)\n",
    "- [Step 7 ‚Äî Build `values_df`](#step27)\n",
    "- [Step 8 ‚Äî Connect to MySQL & Create DB](#step28)\n",
    "- [Step 9 ‚Äî Reset Schema & Create Tables](#step29)\n",
    "- [Step 10 ‚Äî Insert Countries & Indicators](#step210)\n",
    "- [Step 11 ‚Äî Bulk Insert `indicator_values`](#step211)\n",
    "- [Step 12 ‚Äî Create SQL View `all_data`](#step212)\n",
    "- [Step 13 ‚Äî Sanity Checks](#step213)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc4893",
   "metadata": {},
   "source": [
    "## STEP 0 ‚Äî Import all required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3071d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "import custom_functions  # cleaning & encoding helpers\n",
    "\n",
    "pd.options.display.max_columns = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364404c",
   "metadata": {},
   "source": [
    "## STEP 1 ‚Äî Define file paths and abbreviations\n",
    "\n",
    "We list the 8 CSV files (health + environmental indicators) using the same paths as in `main.ipynb`.\n",
    "\n",
    "They are split into:\n",
    "- **Notation 1:** semicolon-delimited, clean files  \n",
    "- **Notation 2:** comma-delimited, messy files requiring extra cleaning  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e47b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for notation 1 ('che', 'wr', 'wu', 'sr', 'su', 'gem')\n",
    "filepaths_1 = [\n",
    "    'che_cleaned.csv',\n",
    "    'wr_cleaned.csv',\n",
    "    'wu_cleaned.csv',\n",
    "    'sr_cleaned.csv',\n",
    "    'su_cleaned.csv',\n",
    "    'gem_cleaned.csv'\n",
    "]\n",
    "\n",
    "# File paths for notation 2 ('pop', 'ren')\n",
    "filepaths_2 = [\n",
    "    'pop_cleaned.csv',\n",
    "    'ren_cleaned.csv'\n",
    "]\n",
    "\n",
    "# Abbreviations\n",
    "abbreviations_1 = ['che', 'wr', 'wu', 'sr', 'su', 'gem']\n",
    "abbreviations_2 = ['pop', 'ren']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d3dfa",
   "metadata": {},
   "source": [
    "## STEP 2 ‚Äî Detect encodings for all CSV files\n",
    "\n",
    "We use `custom_functions.detect_encoding()` to ensure each file is read with the correct encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeff319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodings for notation 1 files:\n",
      " {'che': 'ascii', 'wr': 'ascii', 'wu': 'ascii', 'sr': 'ascii', 'su': 'ascii', 'gem': 'ascii'}\n",
      "Encodings for notation 2 files:\n",
      " {'pop': 'ascii', 'ren': 'ascii'}\n"
     ]
    }
   ],
   "source": [
    "encodings_1 = {}\n",
    "for abbreviation, filepath in zip(abbreviations_1, filepaths_1):\n",
    "    encodings_1[abbreviation] = custom_functions.detect_encoding(filepath)\n",
    "\n",
    "print(\"Encodings for notation 1 files:\\n\", encodings_1)\n",
    "\n",
    "encodings_2 = {}\n",
    "for abbreviation, filepath in zip(abbreviations_2, filepaths_2):\n",
    "    encodings_2[abbreviation] = custom_functions.detect_encoding(filepath)\n",
    "\n",
    "print(\"Encodings for notation 2 files:\\n\", encodings_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1345ee1c",
   "metadata": {},
   "source": [
    "## STEP 3 ‚Äî Clean all CSV files using your custom functions\n",
    "\n",
    "Notation 1 uses:\n",
    "- `;` separator\n",
    "- rows start at index 3\n",
    "\n",
    "Notation 2 uses:\n",
    "- `,` separator\n",
    "- many quotes, commas, extra characters\n",
    "- rows start at index 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299105fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Notation 1\n",
    "dfnames_1 = ['df_che', 'df_wr', 'df_wu', 'df_sr', 'df_su', 'df_gem']\n",
    "df_dict_1 = {}\n",
    "\n",
    "for df_name, abbreviation, filepath in zip(dfnames_1, abbreviations_1, filepaths_1):\n",
    "    df_dict_1[df_name] = custom_functions.clean_csv(\n",
    "        filepath=filepath,\n",
    "        encoding=encodings_1[abbreviation],\n",
    "        separator=',',\n",
    "        trail1='\\n',\n",
    "        trail2=None,\n",
    "        trail3=None,\n",
    "        to_be_replaced='\"',\n",
    "        start_row=3\n",
    "    )\n",
    "\n",
    "df_che = df_dict_1['df_che']\n",
    "df_wr  = df_dict_1['df_wr']\n",
    "df_wu  = df_dict_1['df_wu']\n",
    "df_sr  = df_dict_1['df_sr']\n",
    "df_su  = df_dict_1['df_su']\n",
    "df_gem = df_dict_1['df_gem']\n",
    "\n",
    "# 3.2 Notation 2\n",
    "dfnames_2 = ['df_pop', 'df_ren']\n",
    "df_dict_2 = {}\n",
    "\n",
    "for df_name, abbreviation, filepath in zip(dfnames_2, abbreviations_2, filepaths_2):\n",
    "    df_dict_2[df_name] = custom_functions.clean_csv(\n",
    "        filepath=filepath,\n",
    "        encoding=encodings_2[abbreviation],\n",
    "        separator=',',\n",
    "        trail1='\\n',\n",
    "        trail2='\"',\n",
    "        trail3=',',\n",
    "        to_be_replaced='\"',\n",
    "        start_row=4\n",
    "    )\n",
    "\n",
    "df_pop = df_dict_2['df_pop']\n",
    "df_ren = df_dict_2['df_ren']\n",
    "\n",
    "print(\"Shapes of cleaned DataFrames:\")\n",
    "for name, df in {\n",
    "    \"che\": df_che, \"wr\": df_wr, \"wu\": df_wu,\n",
    "    \"sr\": df_sr, \"su\": df_su, \"gem\": df_gem,\n",
    "    \"pop\": df_pop, \"ren\": df_ren\n",
    "}.items():\n",
    "    print(f\"{name}: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d20195",
   "metadata": {},
   "source": [
    "## STEP 4 ‚Äî Convert all datasets into a unified long-format table\n",
    "\n",
    "Each dataset is converted from wide (1960‚Äì2024 columns) into long format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aebd4da9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_che' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m headers_1 = {\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mche\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mdf_che\u001b[49m.columns.tolist(),\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mwr\u001b[39m\u001b[33m'\u001b[39m: df_wr.columns.tolist(),\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mwu\u001b[39m\u001b[33m'\u001b[39m: df_wu.columns.tolist(),\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msr\u001b[39m\u001b[33m'\u001b[39m: df_sr.columns.tolist(),\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msu\u001b[39m\u001b[33m'\u001b[39m: df_su.columns.tolist(),\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mgem\u001b[39m\u001b[33m'\u001b[39m: df_gem.columns.tolist()\n\u001b[32m      8\u001b[39m }\n\u001b[32m      9\u001b[39m headers_2 = {\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpop\u001b[39m\u001b[33m'\u001b[39m: df_pop.columns.tolist(),\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mren\u001b[39m\u001b[33m'\u001b[39m: df_ren.columns.tolist()\n\u001b[32m     12\u001b[39m }\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mHeaders of cleaned DataFrames:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_che' is not defined"
     ]
    }
   ],
   "source": [
    "headers_1 = {\n",
    "    'che': df_che.columns.tolist(),\n",
    "    'wr': df_wr.columns.tolist(),\n",
    "    'wu': df_wu.columns.tolist(),\n",
    "    'sr': df_sr.columns.tolist(),\n",
    "    'su': df_su.columns.tolist(),\n",
    "    'gem': df_gem.columns.tolist()\n",
    "}\n",
    "headers_2 = {\n",
    "    'pop': df_pop.columns.tolist(),\n",
    "    'ren': df_ren.columns.tolist()\n",
    "}\n",
    "print(\"Headers of cleaned DataFrames:\")\n",
    "for name, headers in {**headers_1, **headers_2}.items():\n",
    "    print(f\"{name}: {headers}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529fcba1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_che' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     29\u001b[39m     long_df[\u001b[33m\"\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m\"\u001b[39m] = long_df[\u001b[33m\"\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m long_df\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m che_long  = melt_indicator(\u001b[43mdf_che\u001b[49m)\n\u001b[32m     34\u001b[39m wr_long   = melt_indicator(df_wr)\n\u001b[32m     35\u001b[39m wu_long   = melt_indicator(df_wu)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_che' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Melting function- Long format\n",
    "\n",
    "def melt_indicator(df):\n",
    "    # 1. FORCE RENAME the first 4 columns to ensure they match id_vars\n",
    "    # This fixes issues with capitalization ('country name') or typos\n",
    "    df.columns.values[0] = \"Country Name\"\n",
    "    df.columns.values[1] = \"Country Code\"\n",
    "    df.columns.values[2] = \"Indicator Name\"\n",
    "    df.columns.values[3] = \"Indicator Code\"\n",
    "\n",
    "    # 2. Identify year columns (all columns except the first 4)\n",
    "    # We use slicing [4:] instead of isdigit() to be safer against bad headers\n",
    "    year_cols = df.columns[4:]\n",
    "\n",
    "    # 3. Melt\n",
    "    long_df = df.melt(\n",
    "        id_vars=[\"Country Name\", \"Country Code\", \"Indicator Name\", \"Indicator Code\"],\n",
    "        value_vars=year_cols,\n",
    "        var_name=\"Year\",\n",
    "        value_name=\"Value\"\n",
    "    )\n",
    "\n",
    "    # 4. Clean up types\n",
    "    long_df[\"Year\"] = pd.to_numeric(long_df[\"Year\"], errors=\"coerce\")\n",
    "    long_df[\"Value\"] = pd.to_numeric(long_df[\"Value\"], errors=\"coerce\")\n",
    "    \n",
    "    # Drop rows where Year is NaN (in case non-year columns were melted)\n",
    "    long_df = long_df.dropna(subset=['Year'])\n",
    "    long_df[\"Year\"] = long_df[\"Year\"].astype(int)\n",
    "    \n",
    "    return long_df\n",
    "\n",
    "che_long  = melt_indicator(df_che)\n",
    "wr_long   = melt_indicator(df_wr)\n",
    "wu_long   = melt_indicator(df_wu)\n",
    "sr_long   = melt_indicator(df_sr)\n",
    "su_long   = melt_indicator(df_su)\n",
    "gem_long  = melt_indicator(df_gem)\n",
    "pop_long  = melt_indicator(df_pop)\n",
    "ren_long  = melt_indicator(df_ren)\n",
    "\n",
    "all_long = pd.concat(\n",
    "    [che_long, wr_long, wu_long, sr_long, su_long, gem_long, pop_long, ren_long],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(\"all_long shape:\", all_long.shape)\n",
    "all_long.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b8bc4",
   "metadata": {},
   "source": [
    "## STEP 5 ‚Äî Build countries_df, indicators_df, values_df\n",
    "\n",
    "These are the **3 tables** that will be inserted into MySQL:\n",
    "- `countries_df` ‚Äî unique list of countries  \n",
    "- `indicators_df` ‚Äî unique list of indicators  \n",
    "- `values_df` ‚Äî all actual data values  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326675c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 countries_df\n",
    "countries_df = (\n",
    "    all_long[[\"Country Code\", \"Country Name\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\"Country Code\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "countries_df[\"country_id\"] = countries_df.index + 1\n",
    "countries_df[\"region\"] = None\n",
    "\n",
    "countries_df = countries_df.rename(columns={\n",
    "    \"Country Code\": \"country_code\",\n",
    "    \"Country Name\": \"country_name\"\n",
    "})\n",
    "\n",
    "print(\"countries_df shape:\", countries_df.shape)\n",
    "countries_df.head()\n",
    "\n",
    "# 5.2 indicators_df\n",
    "indicators_df = (\n",
    "    all_long[[\"Indicator Code\", \"Indicator Name\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "indicators_df[\"indicator_id\"] = indicators_df.index + 1\n",
    "\n",
    "def extract_unit(name):\n",
    "    m = re.search(r\"\\((.*?)\\)\", str(name))\n",
    "    return m.group(1) if m else \"original units\"\n",
    "\n",
    "indicators_df[\"unit\"] = indicators_df[\"Indicator Name\"].apply(extract_unit)\n",
    "\n",
    "indicators_df = indicators_df.rename(columns={\n",
    "    \"Indicator Code\": \"indicator_code\",\n",
    "    \"Indicator Name\": \"indicator_name\"\n",
    "})\n",
    "\n",
    "print(\"indicators_df shape:\", indicators_df.shape)\n",
    "indicators_df.head()\n",
    "\n",
    "# 5.3 values_df\n",
    "country_code_to_id   = dict(zip(countries_df[\"country_code\"],  countries_df[\"country_id\"]))\n",
    "indicator_code_to_id = dict(zip(indicators_df[\"indicator_code\"], indicators_df[\"indicator_id\"]))\n",
    "\n",
    "values_df = all_long.copy()\n",
    "values_df[\"country_id\"]   = values_df[\"Country Code\"].map(country_code_to_id)\n",
    "values_df[\"indicator_id\"] = values_df[\"Indicator Code\"].map(indicator_code_to_id)\n",
    "\n",
    "values_df = values_df.rename(columns={\"Year\": \"year\", \"Value\": \"value\"})\n",
    "\n",
    "print(\"values_df shape:\", values_df.shape)\n",
    "values_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b6886",
   "metadata": {},
   "source": [
    "## STEP 6 ‚Äî Connect to MySQL and create database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e649fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conect to MySQL \n",
    "conn = pymysql.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    user=\"root\",\n",
    "    password=\"student\",   # ‚Üê your real password\n",
    "    autocommit=True\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS bigdata_project;\")\n",
    "cursor.execute(\"USE bigdata_project;\")\n",
    "\n",
    "print(\"Connected to MySQL and using database bigdata_project.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef260d",
   "metadata": {},
   "source": [
    "## STEP 7 ‚Äî Reset tables and recreate schema\n",
    "We drop:\n",
    "- indicator_values  \n",
    "- indicators  \n",
    "- countries  \n",
    "\n",
    "Then recreate all three.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42bc859",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SET FOREIGN_KEY_CHECKS = 0;\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS indicator_values;\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS indicators;\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS countries;\")\n",
    "cursor.execute(\"SET FOREIGN_KEY_CHECKS = 1;\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE countries (\n",
    "    country_id INT PRIMARY KEY,\n",
    "    country_code VARCHAR(5),\n",
    "    country_name VARCHAR(255),\n",
    "    region VARCHAR(100)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE indicators (\n",
    "    indicator_id INT PRIMARY KEY,\n",
    "    indicator_code VARCHAR(255),\n",
    "    indicator_name VARCHAR(500),\n",
    "    unit VARCHAR(100)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE indicator_values (\n",
    "    value_id BIGINT AUTO_INCREMENT PRIMARY KEY,\n",
    "    country_id INT,\n",
    "    indicator_id INT,\n",
    "    year INT,\n",
    "    value DOUBLE,\n",
    "    FOREIGN KEY (country_id) REFERENCES countries(country_id),\n",
    "    FOREIGN KEY (indicator_id) REFERENCES indicators(indicator_id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(\"Tables countries, indicators, indicator_values created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf87894",
   "metadata": {},
   "source": [
    "## STEP 8 ‚Äî Insert countries and indicators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a3afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_countries_sql = \"\"\"\n",
    "    INSERT INTO countries (country_id, country_code, country_name, region)\n",
    "    VALUES (%s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "for _, row in countries_df.iterrows():\n",
    "    cursor.execute(insert_countries_sql, (int(row.country_id), row[\"country_code\"], row[\"country_name\"], row[\"region\"]))\n",
    "\n",
    "print(\"Inserted countries:\", len(countries_df))\n",
    "\n",
    "print(\"\\nüì§ Inserting indicators...\")\n",
    "\n",
    "insert_indicators_sql = \"\"\"\n",
    "    INSERT INTO indicators (indicator_id, indicator_code, indicator_name, unit)\n",
    "    VALUES (%s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "for _, row in indicators_df.iterrows():\n",
    "    cursor.execute(insert_indicators_sql, (int(row.indicator_id), row[\"indicator_code\"], row[\"indicator_name\"], row[\"unit\"]))\n",
    "\n",
    "print(\"Inserted indicators:\", len(indicators_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10108626",
   "metadata": {},
   "source": [
    "## STEP 9 ‚Äî Insert ~100,000 indicator_values in batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"TRUNCATE TABLE indicator_values;\")\n",
    "\n",
    "rows = []\n",
    "for row in values_df.itertuples(index=False):\n",
    "    val = None if pd.isna(row.value) else row.value\n",
    "    rows.append((int(row.country_id), int(row.indicator_id), int(row.year), val))\n",
    "\n",
    "total = len(rows)\n",
    "print(\"Total rows to insert into indicator_values:\", total)\n",
    "\n",
    "insert_values_sql = \"\"\"\n",
    "    INSERT INTO indicator_values (country_id, indicator_id, year, value)\n",
    "    VALUES (%s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "conn.autocommit(False)\n",
    "batch_size = 5000\n",
    "inserted = 0\n",
    "\n",
    "for start in range(0, total, batch_size):\n",
    "    batch = rows[start:start + batch_size]\n",
    "    cursor.executemany(insert_values_sql, batch)\n",
    "    conn.commit()\n",
    "    inserted += len(batch)\n",
    "    print(f\"Inserted {inserted} / {total} rows...\", end=\"\\r\")\n",
    "\n",
    "conn.autocommit(True)\n",
    "print(f\"\\nFinished inserting {inserted} rows into indicator_values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa95e64",
   "metadata": {},
   "source": [
    "## STEP 10 ‚Äî Create view *all_data*\n",
    "\n",
    "This view joins all three tables into a single logical dataset that you can query directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33c55ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW all_data AS\n",
    "SELECT\n",
    "    iv.value_id,\n",
    "    iv.year,\n",
    "    iv.value,\n",
    "    c.country_id,\n",
    "    c.country_code,\n",
    "    c.country_name,\n",
    "    c.region,\n",
    "    i.indicator_id,\n",
    "    i.indicator_code,\n",
    "    i.indicator_name,\n",
    "    i.unit\n",
    "FROM indicator_values iv\n",
    "JOIN countries  c ON iv.country_id   = c.country_id\n",
    "JOIN indicators i ON iv.indicator_id = i.indicator_id;\n",
    "\"\"\")\n",
    "\n",
    "print(\"View all_data created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef2419",
   "metadata": {},
   "source": [
    "## STEP 11 ‚Äî Sanity checks\n",
    "We count rows and preview the joined dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f47384",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Sanity checks:\")\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM countries;\")\n",
    "print(\"countries rows:\", cursor.fetchone()[0])\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM indicators;\")\n",
    "print(\"indicators rows:\", cursor.fetchone()[0])\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM indicator_values;\")\n",
    "print(\"indicator_values rows:\", cursor.fetchone()[0])\n",
    "\n",
    "sample_df = pd.read_sql(\"SELECT * FROM all_data LIMIT 10;\", conn)\n",
    "display(sample_df)\n",
    "\n",
    "print(\"\\n Database setup completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c3f756",
   "metadata": {},
   "source": [
    "## Step 12 - Introducing regions back to the DB\n",
    "\n",
    "Using the resources from https://www.worldbank.org/ext/en/where-we-work we were able to re-introduce regionality as dimension in our database. The accompanying region list excel file was used to intergate the regions back to our mySQL database. \"Countries\" such as Arab World or Europe where labeled as Regions, while ones such as Low Income Post-Demographic Divident where labeled as Socioeconomic Groups. Both of those labels can be utilized as reference and comparison points in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c295fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "REGION_FILE = \"region_list.csv\"\n",
    "\n",
    "# 1. LOAD THE CORRECTED MAPPING FILE\n",
    "try:\n",
    "    # Load the CSV. It now contains the Country Code and the new Region.\n",
    "    df_regions = pd.read_csv(REGION_FILE)\n",
    "    \n",
    "    # Clean the code/region values to ensure no trailing spaces cause SQL errors\n",
    "    df_regions['country_code'] = df_regions['country_code'].astype(str).str.strip()\n",
    "    df_regions['region'] = df_regions['region'].astype(str).str.strip()\n",
    "\n",
    "    # 2. FILTER OUT AGGREGATES\n",
    "    # We only update rows where a region was actually assigned (not 'NULL/IGNORE' or 'REGION')\n",
    "    df_map_ready = df_regions\n",
    "    \n",
    "    print(f\"Loaded mapping for {len(df_map_ready)} valid rows.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load {REGION_FILE}. Please check the filename.\")\n",
    "    raise e\n",
    "\n",
    "# 3. CONNECT AND EXECUTE UPDATES\n",
    "conn = pymysql.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    user=\"root\",\n",
    "    password=\"student\",\n",
    "    database=\"project6001-db\",\n",
    "    autocommit=True\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    print(\"Executing final database update...\")\n",
    "    \n",
    "    update_count = 0\n",
    "    # Loop through the cleaned DataFrame and run the SQL UPDATE for each country\n",
    "    for index, row in df_map_ready.iterrows():\n",
    "        sql = \"UPDATE countries SET region = %s WHERE country_id = %s\"\n",
    "        \n",
    "        # Use the 'region' column as the value to SET, and 'country_code' for the WHERE clause\n",
    "        cursor.execute(sql, (row['region'], row['country_code']))\n",
    "        update_count += 1\n",
    "        \n",
    "    conn.commit()\n",
    "\n",
    "    # 4. VERIFICATION\n",
    "    cursor.execute(\"SELECT region, COUNT(*) FROM countries WHERE region IS NOT NULL GROUP BY region\")\n",
    "    \n",
    "    print(f\"\\nSUCCESS! Updated {update_count} country records.\")\n",
    "    \n",
    "    # Display final region counts\n",
    "    df_verification = pd.DataFrame(cursor.fetchall(), columns=['Region', 'Count'])\n",
    "    print(\"\\nFinal Region Distribution in DB:\")\n",
    "    print(df_verification)\n",
    "\n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
