{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0fc4893",
   "metadata": {},
   "source": [
    "## STEP 0 — Import all required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f3071d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "import custom_functions  # cleaning & encoding helpers\n",
    "\n",
    "pd.options.display.max_columns = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364404c",
   "metadata": {},
   "source": [
    "## STEP 1 — Define file paths and abbreviations\n",
    "\n",
    "We list the 8 CSV files (health + environmental indicators) using the same paths as in `main.ipynb`.\n",
    "\n",
    "They are split into:\n",
    "- **Notation 1:** semicolon-delimited, clean files  \n",
    "- **Notation 2:** comma-delimited, messy files requiring extra cleaning  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e47b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns in region_list.csv: ['country_id', 'country_name', 'country_code', 'region']\n",
      "   country_id          country_name country_code  \\\n",
      "0           1                Angola          AGO   \n",
      "1           2               Albania          ALB   \n",
      "2           3               Andorra          AND   \n",
      "3           4            Arab World          ARB   \n",
      "4           5  United Arab Emirates          ARE   \n",
      "\n",
      "                                              region  \n",
      "0                                             Africa  \n",
      "1                            Europe and Central Asia  \n",
      "2                            Europe and Central Asia  \n",
      "3                                             REGION  \n",
      "4  Middle East, North Africa, Afghanistan & Pakistan  \n"
     ]
    }
   ],
   "source": [
    "# Load the regions file \n",
    "regions_path = \"region_list.csv\"\n",
    "regions_df = pd.read_csv(regions_path)\n",
    "\n",
    "print(\"Original columns in region_list.csv:\", regions_df.columns.tolist())\n",
    "print(regions_df.head())\n",
    "\n",
    "# Base folder where you keep the cleaned CSVs\n",
    "DATA_DIR = \"CLEANED_DATA\"\n",
    "\n",
    "# File paths for notation 1 ('che', 'wr', 'wu', 'sr', 'su', 'gem')\n",
    "filepaths_1 = [\n",
    "    os.path.join(DATA_DIR, \"che_cleaned.csv\"),\n",
    "    os.path.join(DATA_DIR, \"wr_cleaned.csv\"),\n",
    "    os.path.join(DATA_DIR, \"wu_cleaned.csv\"),\n",
    "    os.path.join(DATA_DIR, \"sr_cleaned.csv\"),\n",
    "    os.path.join(DATA_DIR, \"su_cleaned.csv\"),\n",
    "    os.path.join(DATA_DIR, \"gem_cleaned.csv\"),\n",
    "]\n",
    "\n",
    "# File paths for notation 2 ('pop', 'ren')\n",
    "filepaths_2 = [\n",
    "    os.path.join(DATA_DIR, \"pop_cleaned.csv\"),\n",
    "    os.path.join(DATA_DIR, \"ren_cleaned.csv\"),\n",
    "]\n",
    "\n",
    "# (unchanged) abbreviations\n",
    "abbreviations_1 = ['che', 'wr', 'wu', 'sr', 'su', 'gem']\n",
    "abbreviations_2 = ['pop', 'ren']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a961219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regions_df preview:\n",
      "  country_code                                             region\n",
      "0          AGO                                             Africa\n",
      "1          ALB                            Europe and Central Asia\n",
      "2          AND                            Europe and Central Asia\n",
      "3          ARB                                             REGION\n",
      "4          ARE  Middle East, North Africa, Afghanistan & Pakistan\n",
      "Connected to MySQL (bigdata_project)\n",
      "Before update – countries with region set: 0\n",
      "Rows updated in countries.region: 261\n",
      "After update – countries with region set: 261\n",
      "\n",
      "Sample of countries with region:\n",
      "('AGO', 'Angola', 'Africa')\n",
      "('ALB', 'Albania', 'Europe and Central Asia')\n",
      "('AND', 'Andorra', 'Europe and Central Asia')\n",
      "('ARB', 'Arab World', 'REGION')\n",
      "('ARE', 'United Arab Emirates', 'Middle East, North Africa, Afghanistan & Pakistan')\n",
      "('ARG', 'Argentina', 'Latin America and the Caribbean')\n",
      "('ARM', 'Armenia', 'Europe and Central Asia')\n",
      "('ASM', 'American Samoa', 'OCEANIA')\n",
      "('ATG', 'Antigua and Barbuda', 'Latin America and the Caribbean')\n",
      "('AUS', 'Australia', 'OCEANIA')\n",
      "\n",
      "✅ Region update completed.\n"
     ]
    }
   ],
   "source": [
    "# ---- 1. Load regions file (already cleaned earlier) ----\n",
    "regions_path = \"region_list.csv\"\n",
    "regions_df = pd.read_csv(regions_path)\n",
    "\n",
    "# Normalize columns\n",
    "regions_df.columns = [c.strip().lower().replace(\" \", \"_\") for c in regions_df.columns]\n",
    "\n",
    "if \"country_code\" in regions_df.columns:\n",
    "    country_col = \"country_code\"\n",
    "elif \"country code\" in regions_df.columns:\n",
    "    country_col = \"country code\"\n",
    "else:\n",
    "    raise ValueError(\"Could not find a 'country_code' column in region_list.csv\")\n",
    "\n",
    "if \"region\" not in regions_df.columns:\n",
    "    raise ValueError(\"Could not find a 'region' column in region_list.csv\")\n",
    "\n",
    "regions_df = (\n",
    "    regions_df[[country_col, \"region\"]]\n",
    "    .drop_duplicates()\n",
    "    .rename(columns={country_col: \"country_code\"})\n",
    ")\n",
    "\n",
    "print(\"regions_df preview:\")\n",
    "print(regions_df.head())\n",
    "\n",
    "# ---- 2. Fresh connection + cursor (IMPORTANT) ----\n",
    "conn = pymysql.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"Jj698093738013!!\",   # your real password\n",
    "    database=\"bigdata_project\",    # make sure we select the right DB\n",
    "    autocommit=True\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "print(\"Connected to MySQL (bigdata_project)\")\n",
    "\n",
    "# Before update: how many countries already have region?\n",
    "cursor.execute(\"SELECT COUNT(*) FROM countries WHERE region IS NOT NULL;\")\n",
    "print(\"Before update – countries with region set:\", cursor.fetchone()[0])\n",
    "\n",
    "# ---- 3. Apply updates ----\n",
    "update_sql = \"\"\"\n",
    "    UPDATE countries\n",
    "    SET region = %s\n",
    "    WHERE country_code = %s;\n",
    "\"\"\"\n",
    "\n",
    "rows_updated = 0\n",
    "for row in regions_df.itertuples(index=False):\n",
    "    region_val = row.region\n",
    "    cc_val     = row.country_code\n",
    "    cursor.execute(update_sql, (region_val, cc_val))\n",
    "    rows_updated += cursor.rowcount  # how many rows actually changed\n",
    "\n",
    "print(\"Rows updated in countries.region:\", rows_updated)\n",
    "\n",
    "# After update: check again\n",
    "cursor.execute(\"SELECT COUNT(*) FROM countries WHERE region IS NOT NULL;\")\n",
    "print(\"After update – countries with region set:\", cursor.fetchone()[0])\n",
    "\n",
    "# Small sample\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT country_code, country_name, region\n",
    "    FROM countries\n",
    "    WHERE region IS NOT NULL\n",
    "    ORDER BY country_code\n",
    "    LIMIT 10;\n",
    "\"\"\")\n",
    "print(\"\\nSample of countries with region:\")\n",
    "for r in cursor.fetchall():\n",
    "    print(r)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"\\n✅ Region update completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb6210ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized columns: ['country_code', 'region']\n",
      "Cleaned regions_df:\n",
      "  country_code                                             region\n",
      "0          AGO                                             Africa\n",
      "1          ALB                            Europe and Central Asia\n",
      "2          AND                            Europe and Central Asia\n",
      "3          ARB                                             REGION\n",
      "4          ARE  Middle East, North Africa, Afghanistan & Pakistan\n",
      "Unique regions: ['Africa' 'Europe and Central Asia' 'REGION'\n",
      " 'Middle East, North Africa, Afghanistan & Pakistan'\n",
      " 'Latin America and the Caribbean' 'OCEANIA' 'South Asia'\n",
      " 'East Asia and Pacific' 'North America' 'SOCIOECONOMIC']\n"
     ]
    }
   ],
   "source": [
    "#Normalize column names and keep only country_code + region\n",
    "\n",
    "# Make all column names lowercase, strip spaces, replace spaces with underscores\n",
    "regions_df.columns = [\n",
    "    c.strip().lower().replace(\" \", \"_\") for c in regions_df.columns\n",
    "]\n",
    "print(\"Normalized columns:\", regions_df.columns.tolist())\n",
    "\n",
    "# Try to figure out which columns are country_code and region\n",
    "if \"country_code\" in regions_df.columns:\n",
    "    country_col = \"country_code\"\n",
    "elif \"country code\" in regions_df.columns:\n",
    "    country_col = \"country code\"\n",
    "else:\n",
    "    raise ValueError(\"Could not find a 'country_code' column in region_list.csv\")\n",
    "\n",
    "if \"region\" not in regions_df.columns:\n",
    "    raise ValueError(\"Could not find a 'region' column in region_list.csv\")\n",
    "\n",
    "# Keep only what we need and drop duplicates\n",
    "regions_df = (\n",
    "    regions_df[[country_col, \"region\"]]\n",
    "    .drop_duplicates()\n",
    "    .rename(columns={country_col: \"country_code\"})\n",
    ")\n",
    "\n",
    "print(\"Cleaned regions_df:\")\n",
    "print(regions_df.head())\n",
    "print(\"Unique regions:\", regions_df[\"region\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a930584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of cleaned DataFrames (from CLEANED_DATA):\n",
      "che: (235, 16)\n",
      "wr: (214, 16)\n",
      "wu: (226, 16)\n",
      "sr: (117, 16)\n",
      "su: (154, 16)\n",
      "gem: (224, 16)\n",
      "pop: (265, 16)\n",
      "ren: (258, 16)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## STEP 2 — Load already-cleaned CSV files\n",
    "#\n",
    "# At this point, we are no longer working with the original messy World Bank CSVs.\n",
    "# Instead, we use the pre-cleaned files:\n",
    "#   - che_cleaned.csv, wr_cleaned.csv, wu_cleaned.csv,\n",
    "#   - sr_cleaned.csv, su_cleaned.csv, gem_cleaned.csv,\n",
    "#   - pop_cleaned.csv, ren_cleaned.csv\n",
    "#\n",
    "# These are standard comma-separated CSVs with a proper header row:\n",
    "# [\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\",\"1960\",\"1961\",...]\n",
    "#\n",
    "# Because they are clean, we do NOT need custom encoding detection or `clean_csv` here.\n",
    "# We can read them directly with `pandas.read_csv`.\n",
    "\n",
    "# Make sure DATA_DIR is already defined above as:\n",
    "# DATA_DIR = \"CLEANED_DATA\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "che_path = os.path.join(DATA_DIR, \"che_cleaned.csv\")\n",
    "wr_path  = os.path.join(DATA_DIR, \"wr_cleaned.csv\")\n",
    "wu_path  = os.path.join(DATA_DIR, \"wu_cleaned.csv\")\n",
    "sr_path  = os.path.join(DATA_DIR, \"sr_cleaned.csv\")\n",
    "su_path  = os.path.join(DATA_DIR, \"su_cleaned.csv\")\n",
    "gem_path = os.path.join(DATA_DIR, \"gem_cleaned.csv\")\n",
    "pop_path = os.path.join(DATA_DIR, \"pop_cleaned.csv\")\n",
    "ren_path = os.path.join(DATA_DIR, \"ren_cleaned.csv\")\n",
    "\n",
    "# Read all cleaned CSVs directly\n",
    "df_che = pd.read_csv(che_path)\n",
    "df_wr  = pd.read_csv(wr_path)\n",
    "df_wu  = pd.read_csv(wu_path)\n",
    "df_sr  = pd.read_csv(sr_path)\n",
    "df_su  = pd.read_csv(su_path)\n",
    "df_gem = pd.read_csv(gem_path)\n",
    "df_pop = pd.read_csv(pop_path)\n",
    "df_ren = pd.read_csv(ren_path)\n",
    "\n",
    "print(\"Shapes of cleaned DataFrames (from CLEANED_DATA):\")\n",
    "for name, df in {\n",
    "    \"che\": df_che, \"wr\": df_wr, \"wu\": df_wu,\n",
    "    \"sr\": df_sr, \"su\": df_su, \"gem\": df_gem,\n",
    "    \"pop\": df_pop, \"ren\": df_ren,\n",
    "}.items():\n",
    "    print(f\"{name}: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b57a7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "che columns:\n",
      "['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n",
      "\n",
      "wr columns:\n",
      "['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n",
      "\n",
      "wu columns:\n",
      "['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n",
      "\n",
      "sr columns:\n",
      "['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n",
      "\n",
      "su columns:\n",
      "['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n",
      "\n",
      "gem columns:\n",
      "['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n",
      "\n",
      "pop columns:\n",
      "['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n",
      "\n",
      "ren columns:\n",
      "['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## STEP 3 — Sanity check: column names of the cleaned files\n",
    "#\n",
    "# We expect each cleaned file to have columns like:\n",
    "#   [\"Country Name\", \"Country Code\", \"Indicator Name\", \"Indicator Code\", \"1960\", \"1961\", ...]\n",
    "# If any file has a different structure, we will see it here.\n",
    "\n",
    "for name, df in {\n",
    "    \"che\": df_che, \"wr\": df_wr, \"wu\": df_wu,\n",
    "    \"sr\": df_sr, \"su\": df_su, \"gem\": df_gem,\n",
    "    \"pop\": df_pop, \"ren\": df_ren,\n",
    "}.items():\n",
    "    print(f\"\\n{name} columns:\")\n",
    "    print(list(df.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d20195",
   "metadata": {},
   "source": [
    "## STEP 4 — Convert all datasets into a unified long-format table\n",
    "\n",
    "Each dataset is converted from wide (1960–2024 columns) into long format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "529fcba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_long shape: (20316, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Indicator Name</th>\n",
       "      <th>Indicator Code</th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Current health expenditure (% of GDP)</td>\n",
       "      <td>SH.XPD.CHEX.GD.ZS</td>\n",
       "      <td>2008</td>\n",
       "      <td>5.886538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Current health expenditure (% of GDP)</td>\n",
       "      <td>SH.XPD.CHEX.GD.ZS</td>\n",
       "      <td>2008</td>\n",
       "      <td>10.256495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Africa Western and Central</td>\n",
       "      <td>AFW</td>\n",
       "      <td>Current health expenditure (% of GDP)</td>\n",
       "      <td>SH.XPD.CHEX.GD.ZS</td>\n",
       "      <td>2008</td>\n",
       "      <td>3.654871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Angola</td>\n",
       "      <td>AGO</td>\n",
       "      <td>Current health expenditure (% of GDP)</td>\n",
       "      <td>SH.XPD.CHEX.GD.ZS</td>\n",
       "      <td>2008</td>\n",
       "      <td>3.322903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Current health expenditure (% of GDP)</td>\n",
       "      <td>SH.XPD.CHEX.GD.ZS</td>\n",
       "      <td>2008</td>\n",
       "      <td>5.509003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Country Name Country Code  \\\n",
       "0  Africa Eastern and Southern          AFE   \n",
       "1                  Afghanistan          AFG   \n",
       "2   Africa Western and Central          AFW   \n",
       "3                       Angola          AGO   \n",
       "4                      Albania          ALB   \n",
       "\n",
       "                          Indicator Name     Indicator Code  Year      Value  \n",
       "0  Current health expenditure (% of GDP)  SH.XPD.CHEX.GD.ZS  2008   5.886538  \n",
       "1  Current health expenditure (% of GDP)  SH.XPD.CHEX.GD.ZS  2008  10.256495  \n",
       "2  Current health expenditure (% of GDP)  SH.XPD.CHEX.GD.ZS  2008   3.654871  \n",
       "3  Current health expenditure (% of GDP)  SH.XPD.CHEX.GD.ZS  2008   3.322903  \n",
       "4  Current health expenditure (% of GDP)  SH.XPD.CHEX.GD.ZS  2008   5.509003  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 4 – Melting function that works with wide OR long cleaned CSVs\n",
    "\n",
    "def melt_indicator(df):\n",
    "    \"\"\"\n",
    "    Convert a World Bank-style DataFrame to a standard long format with columns:\n",
    "    Country Name, Country Code, Indicator Name, Indicator Code, Year, Value.\n",
    "\n",
    "    It handles two cases:\n",
    "    1. Wide format (many year columns: 1960, 1961, ...)\n",
    "    2. Already-long format (has 'Year' and 'Value' columns)\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    # ---- Case A: already LONG format (has Year + Value) ----\n",
    "    if \"Year\" in cols and \"Value\" in cols:\n",
    "        df2 = df.copy()\n",
    "\n",
    "        # guess meta columns: everything except Year & Value (take first four)\n",
    "        meta_guess = [c for c in cols if c not in [\"Year\", \"Value\"]][:4]\n",
    "\n",
    "        # if there are fewer than 4, we still rename what we have\n",
    "        rename_map = {}\n",
    "        standard_meta = [\"Country Name\", \"Country Code\", \"Indicator Name\", \"Indicator Code\"]\n",
    "        for i, col in enumerate(meta_guess):\n",
    "            rename_map[col] = standard_meta[i]\n",
    "\n",
    "        df2 = df2.rename(columns=rename_map)\n",
    "\n",
    "        # Make sure Year/Value are numeric\n",
    "        df2[\"Year\"] = pd.to_numeric(df2[\"Year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df2[\"Value\"] = pd.to_numeric(df2[\"Value\"], errors=\"coerce\")\n",
    "\n",
    "        # Ensure we return the standard columns (where they exist)\n",
    "        needed_cols = [\"Country Name\", \"Country Code\", \"Indicator Name\", \"Indicator Code\", \"Year\", \"Value\"]\n",
    "        existing = [c for c in needed_cols if c in df2.columns]\n",
    "        return df2[existing]\n",
    "\n",
    "    # ---- Case B: WIDE format (many year columns) ----\n",
    "    # Identify first 4 metadata columns\n",
    "    if len(cols) < 4:\n",
    "        raise ValueError(f\"DataFrame has too few columns to interpret as wide or long: {cols}\")\n",
    "\n",
    "    meta_cols = cols[:4]  # assume first 4 are metadata\n",
    "\n",
    "    rename_map = {\n",
    "        meta_cols[0]: \"Country Name\",\n",
    "        meta_cols[1]: \"Country Code\",\n",
    "        meta_cols[2]: \"Indicator Name\",\n",
    "        meta_cols[3]: \"Indicator Code\",\n",
    "    }\n",
    "\n",
    "    df2 = df.rename(columns=rename_map).copy()\n",
    "\n",
    "    # Year columns = those that look like four-digit years\n",
    "    year_cols = [c for c in df2.columns if re.fullmatch(r\"\\d{4}\", str(c))]\n",
    "\n",
    "    if not year_cols:\n",
    "        raise ValueError(\n",
    "            f\"No 4-digit year columns found in DataFrame. Columns are:\\n{df2.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    long_df = df2.melt(\n",
    "        id_vars=[\"Country Name\", \"Country Code\", \"Indicator Name\", \"Indicator Code\"],\n",
    "        value_vars=year_cols,\n",
    "        var_name=\"Year\",\n",
    "        value_name=\"Value\"\n",
    "    )\n",
    "\n",
    "    long_df[\"Year\"] = long_df[\"Year\"].astype(int)\n",
    "    long_df[\"Value\"] = pd.to_numeric(long_df[\"Value\"], errors=\"coerce\")\n",
    "\n",
    "    return long_df\n",
    "\n",
    "\n",
    "# Apply to all 8 cleaned DataFrames\n",
    "che_long  = melt_indicator(df_che)\n",
    "wr_long   = melt_indicator(df_wr)\n",
    "wu_long   = melt_indicator(df_wu)\n",
    "sr_long   = melt_indicator(df_sr)\n",
    "su_long   = melt_indicator(df_su)\n",
    "gem_long  = melt_indicator(df_gem)\n",
    "pop_long  = melt_indicator(df_pop)\n",
    "ren_long  = melt_indicator(df_ren)\n",
    "\n",
    "# Concatenate everything\n",
    "all_long = pd.concat(\n",
    "    [che_long, wr_long, wu_long, sr_long, su_long, gem_long, pop_long, ren_long],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(\"all_long shape:\", all_long.shape)\n",
    "all_long.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b8bc4",
   "metadata": {},
   "source": [
    "## STEP 5 — Build `countries_df`, `indicators_df`, `values_df` (with regions)\n",
    "\n",
    "In this step we construct the three logical tables that will later be inserted into MySQL:\n",
    "\n",
    "- `countries_df` – one row per country, with a stable `country_id`, ISO country code, country name, and **region**.\n",
    "- `indicators_df` – one row per indicator, with a stable `indicator_id`, code, name, and a simple extracted unit.\n",
    "- `values_df` – all numeric values (facts) in long format: `(country_id, indicator_id, year, value)`.\n",
    "\n",
    "We additionally integrate an external file:\n",
    "\n",
    "- `region_list.csv` – a mapping from `country_code` → `region`.\n",
    "\n",
    "This file is merged into `countries_df` *before* inserting into MySQL, so that the final database and the `all_data` view contain region information for each country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "326675c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base countries_df (before regions) shape: (265, 3)\n",
      "  country_code                 country_name  country_id\n",
      "0          ABW                        Aruba           1\n",
      "1          AFE  Africa Eastern and Southern           2\n",
      "2          AFG                  Afghanistan           3\n",
      "3          AFW   Africa Western and Central           4\n",
      "4          AGO                       Angola           5\n",
      "\n",
      "regions_df preview:\n",
      "  country_code                                             region\n",
      "0          AGO                                             Africa\n",
      "1          ALB                            Europe and Central Asia\n",
      "2          AND                            Europe and Central Asia\n",
      "3          ARB                                             REGION\n",
      "4          ARE  Middle East, North Africa, Afghanistan & Pakistan\n",
      "\n",
      "Countries with non-null region (in DataFrame): 261\n",
      "countries_df (after merge with regions) sample:\n",
      "  country_code                 country_name  country_id  region\n",
      "0          ABW                        Aruba           1     NaN\n",
      "1          AFE  Africa Eastern and Southern           2     NaN\n",
      "2          AFG                  Afghanistan           3     NaN\n",
      "3          AFW   Africa Western and Central           4     NaN\n",
      "4          AGO                       Angola           5  Africa\n",
      "\n",
      "indicators_df shape: (8, 4)\n",
      "      indicator_code                                     indicator_name  \\\n",
      "0  SH.XPD.CHEX.GD.ZS              Current health expenditure (% of GDP)   \n",
      "1  SH.H2O.BASW.RU.ZS  People using at least basic drinking water ser...   \n",
      "2  SH.H2O.BASW.UR.ZS  People using at least basic drinking water ser...   \n",
      "3  SH.STA.SMSS.RU.ZS  People using safely managed sanitation service...   \n",
      "4  SH.STA.SMSS.UR.ZS  People using safely managed sanitation service...   \n",
      "\n",
      "   indicator_id                   unit  \n",
      "0             1               % of GDP  \n",
      "1             2  % of rural population  \n",
      "2             3  % of urban population  \n",
      "3             4  % of rural population  \n",
      "4             5  % of urban population  \n",
      "\n",
      "values_df shape: (20316, 4)\n",
      "   country_id  indicator_id  year      value\n",
      "0           2             1  2008   5.886538\n",
      "1           3             1  2008  10.256495\n",
      "2           4             1  2008   3.654871\n",
      "3           5             1  2008   3.322903\n",
      "4           6             1  2008   5.509003\n"
     ]
    }
   ],
   "source": [
    "# STEP 5 — Build countries_df, indicators_df, values_df (with regions)\n",
    "\n",
    "# 5.1 Build base countries_df from all_long\n",
    "countries_df = (\n",
    "    all_long[[\"Country Code\", \"Country Name\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\"Country Code\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Create a stable integer key\n",
    "countries_df[\"country_id\"] = countries_df.index + 1\n",
    "\n",
    "# Rename columns to database-friendly names\n",
    "countries_df = countries_df.rename(columns={\n",
    "    \"Country Code\": \"country_code\",\n",
    "    \"Country Name\": \"country_name\"\n",
    "})\n",
    "\n",
    "print(\"Base countries_df (before regions) shape:\", countries_df.shape)\n",
    "print(countries_df.head())\n",
    "\n",
    "\n",
    "# 5.2 Integrate region information from region_list.csv\n",
    "\n",
    "# The file region_list.csv should live in the same folder as this notebook\n",
    "# and contain at least two columns: country_code, region\n",
    "regions_path = \"region_list.csv\"\n",
    "regions_df = pd.read_csv(regions_path)\n",
    "\n",
    "# Normalize column names (lowercase, underscores)\n",
    "regions_df.columns = [c.strip().lower().replace(\" \", \"_\") for c in regions_df.columns]\n",
    "\n",
    "if \"country_code\" not in regions_df.columns or \"region\" not in regions_df.columns:\n",
    "    raise ValueError(\n",
    "        \"region_list.csv must contain columns named 'country_code' and 'region'. \"\n",
    "        f\"Found columns: {regions_df.columns.tolist()}\"\n",
    "    )\n",
    "\n",
    "# Keep only the mapping we care about and drop duplicates\n",
    "regions_df = (\n",
    "    regions_df[[\"country_code\", \"region\"]]\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "print(\"\\nregions_df preview:\")\n",
    "print(regions_df.head())\n",
    "\n",
    "# Left-join: keep all countries, add region where we have a match\n",
    "countries_df = countries_df.merge(\n",
    "    regions_df,\n",
    "    on=\"country_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"\\nCountries with non-null region (in DataFrame):\",\n",
    "      countries_df[\"region\"].notna().sum())\n",
    "\n",
    "print(\"countries_df (after merge with regions) sample:\")\n",
    "print(countries_df.head())\n",
    "\n",
    "\n",
    "# 5.3 Build indicators_df\n",
    "\n",
    "indicators_df = (\n",
    "    all_long[[\"Indicator Code\", \"Indicator Name\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "indicators_df[\"indicator_id\"] = indicators_df.index + 1\n",
    "\n",
    "def extract_unit(name):\n",
    "    \"\"\"\n",
    "    Simple heuristic: extract whatever is inside parentheses (...) as the unit.\n",
    "    If none found, we label it 'original units'.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"\\((.*?)\\)\", str(name))\n",
    "    return m.group(1) if m else \"original units\"\n",
    "\n",
    "indicators_df[\"unit\"] = indicators_df[\"Indicator Name\"].apply(extract_unit)\n",
    "\n",
    "indicators_df = indicators_df.rename(columns={\n",
    "    \"Indicator Code\": \"indicator_code\",\n",
    "    \"Indicator Name\": \"indicator_name\"\n",
    "})\n",
    "\n",
    "print(\"\\nindicators_df shape:\", indicators_df.shape)\n",
    "print(indicators_df.head())\n",
    "\n",
    "\n",
    "# 5.4 Build values_df (fact table) using IDs from countries_df and indicators_df\n",
    "\n",
    "country_code_to_id   = dict(zip(countries_df[\"country_code\"],  countries_df[\"country_id\"]))\n",
    "indicator_code_to_id = dict(zip(indicators_df[\"indicator_code\"], indicators_df[\"indicator_id\"]))\n",
    "\n",
    "values_df = all_long.copy()\n",
    "values_df[\"country_id\"]   = values_df[\"Country Code\"].map(country_code_to_id)\n",
    "values_df[\"indicator_id\"] = values_df[\"Indicator Code\"].map(indicator_code_to_id)\n",
    "\n",
    "# Keep only the columns needed for the fact table\n",
    "values_df = values_df[[\"country_id\", \"indicator_id\", \"Year\", \"Value\"]]\n",
    "values_df = values_df.rename(columns={\"Year\": \"year\", \"Value\": \"value\"})\n",
    "\n",
    "print(\"\\nvalues_df shape:\", values_df.shape)\n",
    "print(values_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b6886",
   "metadata": {},
   "source": [
    "## STEP 6 — Connect to MySQL and create database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e649fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL and using database bigdata_project.\n"
     ]
    }
   ],
   "source": [
    "# Conect to MySQL \n",
    "conn = pymysql.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"Jj698093738013!!\",   # ← your real password\n",
    "    autocommit=True\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS bigdata_project;\")\n",
    "cursor.execute(\"USE bigdata_project;\")\n",
    "\n",
    "print(\"Connected to MySQL and using database bigdata_project.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef260d",
   "metadata": {},
   "source": [
    "## STEP 7 — Reset tables and recreate schema\n",
    "We drop:\n",
    "- indicator_values  \n",
    "- indicators  \n",
    "- countries  \n",
    "\n",
    "Then recreate all three.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e42bc859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables countries, indicators, indicator_values created.\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SET FOREIGN_KEY_CHECKS = 0;\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS indicator_values;\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS indicators;\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS countries;\")\n",
    "cursor.execute(\"SET FOREIGN_KEY_CHECKS = 1;\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE countries (\n",
    "    country_id INT PRIMARY KEY,\n",
    "    country_code VARCHAR(5),\n",
    "    country_name VARCHAR(255),\n",
    "    region VARCHAR(100)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE indicators (\n",
    "    indicator_id INT PRIMARY KEY,\n",
    "    indicator_code VARCHAR(255),\n",
    "    indicator_name VARCHAR(500),\n",
    "    unit VARCHAR(100)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE indicator_values (\n",
    "    value_id BIGINT AUTO_INCREMENT PRIMARY KEY,\n",
    "    country_id INT,\n",
    "    indicator_id INT,\n",
    "    year INT,\n",
    "    value DOUBLE,\n",
    "    FOREIGN KEY (country_id) REFERENCES countries(country_id),\n",
    "    FOREIGN KEY (indicator_id) REFERENCES indicators(indicator_id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(\"Tables countries, indicators, indicator_values created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf87894",
   "metadata": {},
   "source": [
    "## STEP 8 — Insert countries and indicators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "324a3afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "nan can not be used with MySQL",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mProgrammingError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m insert_countries_sql = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33m    INSERT INTO countries (country_id, country_code, country_name, region)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33m    VALUES (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m);\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m countries_df.iterrows():\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minsert_countries_sql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcountry_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcountry_code\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcountry_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mregion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInserted countries:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(countries_df))\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Inserting indicators...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymysql\\cursors.py:151\u001b[39m, in \u001b[36mCursor.execute\u001b[39m\u001b[34m(self, query, args)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nextset():\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m query = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmogrify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m result = \u001b[38;5;28mself\u001b[39m._query(query)\n\u001b[32m    154\u001b[39m \u001b[38;5;28mself\u001b[39m._executed = query\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymysql\\cursors.py:129\u001b[39m, in \u001b[36mCursor.mogrify\u001b[39m\u001b[34m(self, query, args)\u001b[39m\n\u001b[32m    126\u001b[39m conn = \u001b[38;5;28mself\u001b[39m._get_db()\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     query = query % \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_escape_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m query\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymysql\\cursors.py:102\u001b[39m, in \u001b[36mCursor._escape_args\u001b[39m\u001b[34m(self, args, conn)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_escape_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, conn):\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mliteral\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: conn.literal(val) \u001b[38;5;28;01mfor\u001b[39;00m (key, val) \u001b[38;5;129;01min\u001b[39;00m args.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymysql\\cursors.py:102\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_escape_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, conn):\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mliteral\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: conn.literal(val) \u001b[38;5;28;01mfor\u001b[39;00m (key, val) \u001b[38;5;129;01min\u001b[39;00m args.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymysql\\connections.py:542\u001b[39m, in \u001b[36mConnection.literal\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mliteral\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj):\n\u001b[32m    538\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Alias for escape().\u001b[39;00m\n\u001b[32m    539\u001b[39m \n\u001b[32m    540\u001b[39m \u001b[33;03m    Non-standard, for internal use; do not use this in your applications.\u001b[39;00m\n\u001b[32m    541\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mescape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoders\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymysql\\connections.py:535\u001b[39m, in \u001b[36mConnection.escape\u001b[39m\u001b[34m(self, obj, mapping)\u001b[39m\n\u001b[32m    533\u001b[39m         ret = \u001b[33m\"\u001b[39m\u001b[33m_binary\u001b[39m\u001b[33m\"\u001b[39m + ret\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverters\u001b[49m\u001b[43m.\u001b[49m\u001b[43mescape_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcharset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymysql\\converters.py:25\u001b[39m, in \u001b[36mescape_item\u001b[39m\u001b[34m(val, charset, mapping)\u001b[39m\n\u001b[32m     23\u001b[39m     val = encoder(val, charset, mapping)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     val = \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymysql\\converters.py:56\u001b[39m, in \u001b[36mescape_float\u001b[39m\u001b[34m(value, mapping)\u001b[39m\n\u001b[32m     54\u001b[39m s = \u001b[38;5;28mrepr\u001b[39m(value)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m-inf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnan\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ProgrammingError(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m can not be used with MySQL\u001b[39m\u001b[33m\"\u001b[39m % s)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33me\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m s:\n\u001b[32m     58\u001b[39m     s += \u001b[33m\"\u001b[39m\u001b[33me0\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mProgrammingError\u001b[39m: nan can not be used with MySQL"
     ]
    }
   ],
   "source": [
    "insert_countries_sql = \"\"\"\n",
    "    INSERT INTO countries (country_id, country_code, country_name, region)\n",
    "    VALUES (%s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "for _, row in countries_df.iterrows():\n",
    "    cursor.execute(insert_countries_sql, (int(row.country_id), row[\"country_code\"], row[\"country_name\"], row[\"region\"]))\n",
    "\n",
    "print(\"Inserted countries:\", len(countries_df))\n",
    "\n",
    "print(\"\\n Inserting indicators...\")\n",
    "\n",
    "insert_indicators_sql = \"\"\"\n",
    "    INSERT INTO indicators (indicator_id, indicator_code, indicator_name, unit)\n",
    "    VALUES (%s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "for _, row in indicators_df.iterrows():\n",
    "    cursor.execute(insert_indicators_sql, (int(row.indicator_id), row[\"indicator_code\"], row[\"indicator_name\"], row[\"unit\"]))\n",
    "\n",
    "print(\"Inserted indicators:\", len(indicators_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10108626",
   "metadata": {},
   "source": [
    "## STEP 9 — Insert ~100,000 indicator_values in batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1397a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows to insert into indicator_values: 20316\n",
      "Inserted 20316 / 20316 rows...\n",
      "Finished inserting 20316 rows into indicator_values.\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"TRUNCATE TABLE indicator_values;\")\n",
    "\n",
    "rows = []\n",
    "for row in values_df.itertuples(index=False):\n",
    "    val = None if pd.isna(row.value) else row.value\n",
    "    rows.append((int(row.country_id), int(row.indicator_id), int(row.year), val))\n",
    "\n",
    "total = len(rows)\n",
    "print(\"Total rows to insert into indicator_values:\", total)\n",
    "\n",
    "insert_values_sql = \"\"\"\n",
    "    INSERT INTO indicator_values (country_id, indicator_id, year, value)\n",
    "    VALUES (%s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "conn.autocommit(False)\n",
    "batch_size = 5000\n",
    "inserted = 0\n",
    "\n",
    "for start in range(0, total, batch_size):\n",
    "    batch = rows[start:start + batch_size]\n",
    "    cursor.executemany(insert_values_sql, batch)\n",
    "    conn.commit()\n",
    "    inserted += len(batch)\n",
    "    print(f\"Inserted {inserted} / {total} rows...\", end=\"\\r\")\n",
    "\n",
    "conn.autocommit(True)\n",
    "print(f\"\\nFinished inserting {inserted} rows into indicator_values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa95e64",
   "metadata": {},
   "source": [
    "## STEP 10 — Create view *all_data*\n",
    "\n",
    "This view joins all three tables into a single logical dataset that you can query directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33c55ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View all_data created.\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW all_data AS\n",
    "SELECT\n",
    "    iv.value_id,\n",
    "    iv.year,\n",
    "    iv.value,\n",
    "    c.country_id,\n",
    "    c.country_code,\n",
    "    c.country_name,\n",
    "    c.region,\n",
    "    i.indicator_id,\n",
    "    i.indicator_code,\n",
    "    i.indicator_name,\n",
    "    i.unit\n",
    "FROM indicator_values iv\n",
    "JOIN countries  c ON iv.country_id   = c.country_id\n",
    "JOIN indicators i ON iv.indicator_id = i.indicator_id;\n",
    "\"\"\")\n",
    "\n",
    "print(\"View all_data created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef2419",
   "metadata": {},
   "source": [
    "## STEP 11 — Sanity checks\n",
    "We count rows and preview the joined dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f47384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sanity checks:\n",
      "countries rows: 265\n",
      "indicators rows: 8\n",
      "indicator_values rows: 20316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jannos\\AppData\\Local\\Temp\\ipykernel_13668\\3130834757.py:12: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sample_df = pd.read_sql(\"SELECT * FROM all_data LIMIT 10;\", conn)\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql: SELECT * FROM all_data LIMIT 10;\n(0, '')\nunable to rollback",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInterfaceError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\sql.py:2664\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2663\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2664\u001b[39m     \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymysql\\cursors.py:153\u001b[39m, in \u001b[36mCursor.execute\u001b[39m\u001b[34m(self, query, args)\u001b[39m\n\u001b[32m    151\u001b[39m query = \u001b[38;5;28mself\u001b[39m.mogrify(query, args)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28mself\u001b[39m._executed = query\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymysql\\cursors.py:322\u001b[39m, in \u001b[36mCursor._query\u001b[39m\u001b[34m(self, q)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28mself\u001b[39m._clear_result()\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[38;5;28mself\u001b[39m._do_get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymysql\\connections.py:574\u001b[39m, in \u001b[36mConnection.query\u001b[39m\u001b[34m(self, sql, unbuffered)\u001b[39m\n\u001b[32m    573\u001b[39m     sql = sql.encode(\u001b[38;5;28mself\u001b[39m.encoding, \u001b[33m\"\u001b[39m\u001b[33msurrogateescape\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOMMAND\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOM_QUERY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;28mself\u001b[39m._affected_rows = \u001b[38;5;28mself\u001b[39m._read_query_result(unbuffered=unbuffered)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymysql\\connections.py:844\u001b[39m, in \u001b[36mConnection._execute_command\u001b[39m\u001b[34m(self, command, sql)\u001b[39m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sock:\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err.InterfaceError(\u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    846\u001b[39m \u001b[38;5;66;03m# If the last query was unbuffered, make sure it finishes before\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# sending new commands\u001b[39;00m\n",
      "\u001b[31mInterfaceError\u001b[39m: (0, '')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mInterfaceError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\sql.py:2668\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2667\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2668\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrollback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2669\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m inner_exc:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymysql\\connections.py:504\u001b[39m, in \u001b[36mConnection.rollback\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[33;03mRoll back the current transaction.\u001b[39;00m\n\u001b[32m    500\u001b[39m \n\u001b[32m    501\u001b[39m \u001b[33;03mSee `Connection.rollback() <https://www.python.org/dev/peps/pep-0249/#rollback>`_\u001b[39;00m\n\u001b[32m    502\u001b[39m \u001b[33;03min the specification.\u001b[39;00m\n\u001b[32m    503\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOMMAND\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOM_QUERY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mROLLBACK\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28mself\u001b[39m._read_ok_packet()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pymysql\\connections.py:844\u001b[39m, in \u001b[36mConnection._execute_command\u001b[39m\u001b[34m(self, command, sql)\u001b[39m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sock:\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err.InterfaceError(\u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    846\u001b[39m \u001b[38;5;66;03m# If the last query was unbuffered, make sure it finishes before\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# sending new commands\u001b[39;00m\n",
      "\u001b[31mInterfaceError\u001b[39m: (0, '')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatabaseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m cursor.execute(\u001b[33m\"\u001b[39m\u001b[33mSELECT COUNT(*) FROM indicator_values;\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mindicator_values rows:\u001b[39m\u001b[33m\"\u001b[39m, cursor.fetchone()[\u001b[32m0\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m sample_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT * FROM all_data LIMIT 10;\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m display(sample_df)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Database setup completed successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\sql.py:708\u001b[39m, in \u001b[36mread_sql\u001b[39m\u001b[34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    720\u001b[39m         _is_table_name = pandas_sql.has_table(sql)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\sql.py:2728\u001b[39m, in \u001b[36mSQLiteDatabase.read_query\u001b[39m\u001b[34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m   2717\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_query\u001b[39m(\n\u001b[32m   2718\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2719\u001b[39m     sql,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2726\u001b[39m     dtype_backend: DtypeBackend | Literal[\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2727\u001b[39m ) -> DataFrame | Iterator[DataFrame]:\n\u001b[32m-> \u001b[39m\u001b[32m2728\u001b[39m     cursor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2729\u001b[39m     columns = [col_desc[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor.description]\n\u001b[32m   2731\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\sql.py:2673\u001b[39m, in \u001b[36mSQLiteDatabase.execute\u001b[39m\u001b[34m(self, sql, params)\u001b[39m\n\u001b[32m   2669\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m inner_exc:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m   2670\u001b[39m     ex = DatabaseError(\n\u001b[32m   2671\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecution failed on sql: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33munable to rollback\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2672\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2673\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minner_exc\u001b[39;00m\n\u001b[32m   2675\u001b[39m ex = DatabaseError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecution failed on sql \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mDatabaseError\u001b[39m: Execution failed on sql: SELECT * FROM all_data LIMIT 10;\n(0, '')\nunable to rollback"
     ]
    }
   ],
   "source": [
    "print(\"\\n Sanity checks:\")\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM countries;\")\n",
    "print(\"countries rows:\", cursor.fetchone()[0])\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM indicators;\")\n",
    "print(\"indicators rows:\", cursor.fetchone()[0])\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM indicator_values;\")\n",
    "print(\"indicator_values rows:\", cursor.fetchone()[0])\n",
    "\n",
    "sample_df = pd.read_sql(\"SELECT * FROM all_data LIMIT 10;\", conn)\n",
    "display(sample_df)\n",
    "\n",
    "print(\"\\n Database setup completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cddbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries with region set (sample):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jannos\\AppData\\Local\\Temp\\ipykernel_13668\\3969881068.py:12: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sample_regions = pd.read_sql(\"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [country_code, country_name, region]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample from all_data with region filled:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jannos\\AppData\\Local\\Temp\\ipykernel_13668\\3969881068.py:23: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sample_all_data = pd.read_sql(\"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>region</th>\n",
       "      <th>indicator_code</th>\n",
       "      <th>year</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [country_code, country_name, region, indicator_code, year, value]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"Jj698093738013!!\",\n",
    "    database=\"bigdata_project\",\n",
    "    autocommit=True\n",
    ")\n",
    "\n",
    "sample_regions = pd.read_sql(\"\"\"\n",
    "    SELECT country_code, country_name, region\n",
    "    FROM countries\n",
    "    WHERE region IS NOT NULL\n",
    "    ORDER BY country_code\n",
    "    LIMIT 20;\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"Countries with region set (sample):\")\n",
    "display(sample_regions)\n",
    "\n",
    "sample_all_data = pd.read_sql(\"\"\"\n",
    "    SELECT country_code, country_name, region, indicator_code, year, value\n",
    "    FROM all_data\n",
    "    WHERE region IS NOT NULL\n",
    "    LIMIT 20;\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\nSample from all_data with region filled:\")\n",
    "display(sample_all_data)\n",
    "\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
